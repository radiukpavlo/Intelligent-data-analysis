{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Лабораторна робота 4.</center></h1>\n",
    "<h2><center>Виявлення сарказму за текстовими повідомленнями з використанням логістичної регресії</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Виконав:** Прізвище І.П.\n",
    "\n",
    "**Варіант:** №__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В цій лабораторній роботі ми будемо подувати модель логістичної регресії для розв'язання задачі класифікації саркастичних текстів. Для цього використаємо набір даних з [наукової статі](https://arxiv.org/abs/1704.05579) «A Large Self-Annotated Corpus for Sarcasm» з понад 1 мільйоном коментарів із платформи Reddit, що позначені авторами статті як саркастичні або ні. Оброблена версія цього набору даних міститься на платформі Kaggle у формі [Набору даних Kaggle](https://www.kaggle.com/datasets/danofer/sarcasm/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зміст\n",
    "\n",
    "- [4.1. Попередній аналіз даних](#lab-4.1)\n",
    "- [4.2. Векторизація TF-IDF та логістична регресія](#lab-4.2)\n",
    "- [4.3. Інтерпретація та порівняння моделі](#lab-4.3)\n",
    "- [4.4. Розширене вдосконалення моделі](#lab-4.4)\n",
    "- [4.5. Практичне застосування результатів інтелектуального аналізу даних](#lab-4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спершу завантажте цільовий набір даних `train-balanced-sarcasm.csv` із ресурсу Kaggle за [посиланням](https://www.kaggle.com/datasets/danofer/sarcasm/data?select=train-balanced-sarcasm.csv). Збережіть файл .csv в теку з робочим файлом лабораторної роботи.\n",
    "\n",
    "Цей набір даних містить 1,3 млн. саркастичних коментарів з платформи інтернет-коментарів Reddit. Набір даних було створено шляхом вилучення коментарів з Reddit, що містять тег `\\s` (\"сарказм\"). Цей тег переважно використовується користувачами, щоб вказати на жартівливий тон їхнього тексту, тобто такий запис не повинен сприйматися серйозно, і, як правило, є надійним показником саркастичного змісту запису.\n",
    "\n",
    "Дані мають збалансовану та незбалансовану (тобто справжній розподіл) версії. Реальне співвідношення несаркастичних до саркастичних коментарів становить приблизно 1:100. За потреби оригінальний набір даних можна отримати за [посиланням](https://github.com/NLPrinceton/SARC).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:38.704867600Z",
     "start_time": "2023-11-21T14:39:38.692862900Z"
    },
    "_uuid": "ed87ab2845921166bb73ca854bfe1ef013c035e9"
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"train-balanced-sarcasm.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:40.010896800Z",
     "start_time": "2023-11-21T14:39:38.706865200Z"
    },
    "_uuid": "ffa03aec57ab6150f9bec0fa56cd3a5791a3e6f4"
   },
   "outputs": [],
   "source": [
    "# Виконаємо завантаження основних бібліотек\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:43.077555Z",
     "start_time": "2023-11-21T14:39:40.012897800Z"
    },
    "_uuid": "b23e4fc7a1973d60e0c6da8bd60f3d921542a856"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(PATH_TO_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нижче переглянемо зразки цільового набору в розрізі основних ознак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:43.092558100Z",
     "start_time": "2023-11-21T14:39:43.078556900Z"
    },
    "_uuid": "4dc7b3787afa46c7eb0d0e33b0c41ab9821c4a27"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                         NC and NH.  Trumpbart   \n",
       "1      0  You do know west teams play against west teams...  Shbshb906   \n",
       "2      0  They were underdogs earlier today, but since G...   Creepeth   \n",
       "3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n",
       "4      0                    I could use one of those tools.  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc  \\\n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:43.818068200Z",
     "start_time": "2023-11-21T14:39:43.095558600Z"
    },
    "_uuid": "0a7ed9557943806c6813ad59c3d5ebdb403ffd78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1010826 entries, 0 to 1010825\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count    Dtype \n",
      "---  ------          --------------    ----- \n",
      " 0   label           1010826 non-null  int64 \n",
      " 1   comment         1010773 non-null  object\n",
      " 2   author          1010826 non-null  object\n",
      " 3   subreddit       1010826 non-null  object\n",
      " 4   score           1010826 non-null  int64 \n",
      " 5   ups             1010826 non-null  int64 \n",
      " 6   downs           1010826 non-null  int64 \n",
      " 7   date            1010826 non-null  object\n",
      " 8   created_utc     1010826 non-null  object\n",
      " 9   parent_comment  1010826 non-null  object\n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 77.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6472f52fb5ecb8bb2a6e3b292678a2042fcfe34c"
   },
   "source": [
    "Деяких коментарів бракує в наборі, тому ми видалимо відповідні рядки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:44.123072100Z",
     "start_time": "2023-11-21T14:39:43.777084800Z"
    },
    "_uuid": "97b2d85627fcde52a506dbdd55d4d6e4c87d3f08"
   },
   "outputs": [],
   "source": [
    "train_df.dropna(subset=[\"comment\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d51637ee70dca7693737ad0da1dbb8c6ce9230b"
   },
   "source": [
    "Переконаємось, що розглядуваний набір даних є справді збалансованим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:44.138071100Z",
     "start_time": "2023-11-21T14:39:44.132073900Z"
    },
    "_uuid": "addd77c640423d30fd146c8d3a012d3c14481e11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    505405\n",
       "1    505368\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b836574e5093c5eb2e9063fefe1c8d198dcba79"
   },
   "source": [
    "Розділимо дані на частини для навчання та валідації."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:44.389073100Z",
     "start_time": "2023-11-21T14:39:44.137067500Z"
    },
    "_uuid": "c200add4e1dcbaa75164bbcc73b9c12ecb863c96"
   },
   "outputs": [],
   "source": [
    "train_texts, valid_texts, y_train, y_valid = train_test_split(\n",
    "    train_df[\"comment\"], train_df[\"label\"], random_state=17\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba1a8f65032c5954476a68e01b607655145b746d"
   },
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">4.1. Попередній аналіз даних</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:44.405070600Z",
     "start_time": "2023-11-21T14:39:44.390070Z"
    },
    "_uuid": "c2c613ee2052a2c0379682adf5c23d1f751f4c3b"
   },
   "outputs": [],
   "source": [
    "# from wordcloud import STOPWORDS, WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:44.442068900Z",
     "start_time": "2023-11-21T14:39:44.409069Z"
    },
    "_uuid": "ae7333d67f6a17673d2aa16aed3017e2fbef9b58"
   },
   "outputs": [],
   "source": [
    "# Підготуємо об'єкт класу WordCloud для \n",
    "# можливого візуального аналізу  \n",
    "# my_wordcloud = WordCloud(\n",
    "#     background_color=\"black\",\n",
    "#     stopwords=STOPWORDS,\n",
    "#     max_words=200,\n",
    "#     max_font_size=100,\n",
    "#     random_state=17,\n",
    "#     width=800,\n",
    "#     height=400,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:2em;\">Завдання 1</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1:** Проаналізуйте структуру текстових даних, включно з кількістю символів та слів у кожному повідомленні. Оцініть, як довжина тексту може вплинути на точність виявлення сарказму. Використайте для цього функції `len()` для обчислення кількості символів та метод `apply(lambda x: len(x.split()))` для обчислення кількості слів у датафреймі Pandas.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2:** Здійсніть лемматизацію та стемінг текстів у наборі даних. Порівняйте ефективність обох методів для виявлення сарказму. Використайте бібліотеку `nltk` для стемінгу через `PorterStemmer`, а для лемматизації скористайтеся `spacy` через функцію `nlp.pipe()` для пакетного оброблення.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3:** Виконайте токенізацію текстів та проаналізуйте частоту вживання окремих слів. Оцініть вплив високочастотних і низькочастотних слів на побудову моделей. Використайте `CountVectorizer` з бібліотеки `sklearn.feature_extraction.text` для обчислення кількості частоти слів.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4:** Проаналізуйте баланс класів у наборі даних. Оцініть наявність дисбалансу між позитивними та негативними прикладами сарказму. Скористайтесь функцією `value_counts()` у Pandas для виведення статистики кожного класу.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5:** Проаналізуйте, як зміна регістру тексту (великі/малі літери) може впливати на класифікацію сарказму. Виконайте нормалізацію регістру за допомогою методу `str.lower()` у Pandas і порівняйте результати до та після цієї обробки.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6:** Проаналізуйте та видаліть стоп-слова з тексту. Оцініть, як видалення стоп-слів впливає на побудову моделей. Використайте стандартні списки стоп-слів із бібліотек `nltk` (`nltk.corpus.stopwords`) або `sklearn.feature_extraction.text.TfidfVectorizer(stop_words='english')`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7:** Виконайте аналіз біграм у текстах. Оцініть, які поєднання слів (біграми) найчастіше трапляються в саркастичних висловлюваннях. Використайте `TfidfVectorizer` з налаштуванням параметра `ngram_range=(2, 2)`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8:** Проаналізуйте вживання числових значень у текстах. Оцініть, як їх наявність може впливати на класифікацію сарказму, і чи є різниця між текстами із числами та без них. Для цього скористайтесь регулярними виразами (`re.findall(r'\\d+', text)`) для пошуку чисел.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9:** Проведіть аналіз частоти вживання знаків пунктуації, таких як крапки, коми, знаки оклику та питання. Визначте, як ці символи корелюють із наявністю сарказму в текстах. Для цього скористайтесь регулярними виразами (`re.findall(r'[?!.,]', text)`).\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10:** Проаналізуйте частоту використання числових значень у текстах (наприклад, дати, години, числа). Оцініть, як їхня наявність може впливати на моделі виявлення сарказму, використовуючи регулярні вирази для пошуку чисел (`re.findall(r'\\d+', text)`).\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11:** Виконайте аналіз частоти появи та впливу цитат і прямих звернень у текстах на результати моделі. Для пошуку таких елементів використайте регулярні вирази (`re.findall(r'\\\"[^\\\"]+\\\"', text)`).\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12:** Проведіть аналіз текстів за розміром шрифтів, наприклад, коли всі літери набрані великим регістром (CAPS LOCK). Використайте метод `str.isupper()` у Pandas для виявлення таких текстів і оцініть їх частоту в наборі даних.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13:** Оцініть вплив символів \"?» і \"!» на визначення сарказму. Виконайте підрахунок частоти їхньої появи за допомогою регулярних виразів і оцініть зв’язок із саркастичними повідомленнями, використовуючи `text.count('!')` або `text.count('?')`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14:** Проаналізуйте та підготуйте набір даних для моделі, що враховує контекстні зв’язки між коментарями та відповідями. Використайте бібліотеку `nltk` для сегментації діалогів та побудуйте модель на основі діалогів, використовуючи відповідні контекстні репліки.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15:** Виконайте аналіз частоти використання абревіатур та скорочень у текстах. Оцініть, як часто вони з’являються в саркастичних висловлюваннях і впливають на продуктивність моделі. Для пошуку скорочень використайте регулярні вирази (`re.findall(r'\\b[A-Z]{2,}\\b', text)`).\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16:** Проведіть аналіз частоти вживання запозичених слів або сленгу. Як ці елементи мови можуть впливати на розпізнавання сарказму? Для цього скористайтесь зовнішніми словниками або бібліотекою `nltk` для пошуку специфічних слів.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17:** Проаналізуйте наявність посилань на інші джерела (URLs) у текстах. Як часто сарказм супроводжується гіперпосиланнями і як це впливає на класифікацію? Використайте регулярний вираз для пошуку URL (`re.findall(r'http\\S+', text)`).\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18:** Виконайте попередній аналіз текстів на наявність повторів або копіювання певних фраз чи виразів. Оцініть, як це може вплинути на точність моделі виявлення сарказму, використовуючи функцію `duplicated()` у Pandas для пошуку дублікатів.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19:** Виконайте аналіз тональності текстів (позитивний, нейтральний, негативний). Оцініть вплив тональності на виявлення сарказму за допомогою моделей аналізу сентименту, таких як `TextBlob` або `VADER` з бібліотеки `nltk`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20:** Проаналізуйте вплив багатомовності на виявлення сарказму в текстах. Чи є суттєві відмінності в саркастичних висловлюваннях різними мовами в змішаному наборі даних? Використайте бібліотеку `langdetect` для виявлення мови тексту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-4.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "416321f19f5a27290bc5622e8b3384b7bbbd28c6"
   },
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">4.2. Векторизація TF-IDF та логістична регресія</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:44.442068900Z",
     "start_time": "2023-11-21T14:39:44.424075300Z"
    },
    "_uuid": "3048a070a56b08eb4e5fe2c54b6d14905031e74a"
   },
   "outputs": [],
   "source": [
    "# Побудуємо біграми, обмежимо максимальну кількість ознак\n",
    "# і мінімальну частоту слів\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 2), max_features=5000, min_df=2)\n",
    "# Створимо об'єкт мультиноміальної логістичної регресії,\n",
    "# яка також відома як класифікатор softmax\n",
    "logit = LogisticRegression(C=1, n_jobs=4, solver=\"lbfgs\", random_state=17, verbose=1)\n",
    "# Задамо sklearn's pipeline\n",
    "tfidf_logit_pipeline = Pipeline([(\"tf_idf\", tf_idf), (\"logit\", logit)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:40:14.349574100Z",
     "start_time": "2023-11-21T14:39:44.500067700Z"
    },
    "_uuid": "8756bac7457218e4daf08ec276211f03971c17fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:    7.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 18.2 s\n",
      "Wall time: 26.9 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tf_idf&#x27;,\n",
       "                 TfidfVectorizer(max_features=5000, min_df=2,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                (&#x27;logit&#x27;,\n",
       "                 LogisticRegression(C=1, n_jobs=4, random_state=17,\n",
       "                                    verbose=1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tf_idf&#x27;,\n",
       "                 TfidfVectorizer(max_features=5000, min_df=2,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                (&#x27;logit&#x27;,\n",
       "                 LogisticRegression(C=1, n_jobs=4, random_state=17,\n",
       "                                    verbose=1))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=5000, min_df=2, ngram_range=(1, 2))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1, n_jobs=4, random_state=17, verbose=1)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tf_idf',\n",
       "                 TfidfVectorizer(max_features=5000, min_df=2,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                ('logit',\n",
       "                 LogisticRegression(C=1, n_jobs=4, random_state=17,\n",
       "                                    verbose=1))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_logit_pipeline.fit(train_texts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:40:18.961782300Z",
     "start_time": "2023-11-21T14:40:14.338567600Z"
    },
    "_uuid": "d2e47f77f999c2fb5aee9ef1de1542bc93de4c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.72 s\n",
      "Wall time: 4.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_pred = tfidf_logit_pipeline.predict(valid_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:40:19.008778Z",
     "start_time": "2023-11-21T14:40:18.963793600Z"
    },
    "_uuid": "a8f93efc3db12910eaa6d7944feebb2418714203"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точність за валідаційним набором даних:      0.6983\n"
     ]
    }
   ],
   "source": [
    "print(f\"Точність за валідаційним набором даних: \\\n",
    "    {accuracy_score(y_valid, valid_pred): .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:40:19.281356900Z",
     "start_time": "2023-11-21T14:40:18.997781500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "# Розбиття набору даних\n",
    "train_texts, valid_texts, y_train, y_valid = train_test_split(\n",
    "    train_df[\"comment\"], train_df[\"label\"], random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:40:19.296352100Z",
     "start_time": "2023-11-21T14:40:19.284354200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Функція для інкапсуляції виконання пайплайну\n",
    "def run_pipeline(train_texts, y_train, valid_texts):\n",
    "    # Налаштування TfidfVectorizer\n",
    "    tf_idf = TfidfVectorizer(ngram_range=(1, 2), max_features=20000, min_df=5)\n",
    "\n",
    "    # Створення Деревa Рішень з обмеженою глибиною\n",
    "    decision_tree = DecisionTreeClassifier(max_depth=10, random_state=17)\n",
    "\n",
    "    # Налаштування пайплайну\n",
    "    tfidf_tree_pipeline = Pipeline([(\"tf_idf\", tf_idf), (\"decision_tree\", decision_tree)])\n",
    "\n",
    "    # Навчання моделі\n",
    "    tfidf_tree_pipeline.fit(train_texts, y_train)\n",
    "\n",
    "    # Виконання прогнозів\n",
    "    valid_pred = tfidf_tree_pipeline.predict(valid_texts)\n",
    "    \n",
    "    return valid_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:41:17.593161300Z",
     "start_time": "2023-11-21T14:40:19.311353300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         35353936 function calls (35353926 primitive calls) in 53.117 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000   53.117   26.559 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3469(run_code)\n",
      "        2    0.000    0.000   53.117   26.559 {built-in method builtins.exec}\n",
      "        1    0.215    0.215   53.117   53.117 C:\\Users\\radiu\\AppData\\Local\\Temp\\ipykernel_11996\\2777130815.py:6(<module>)\n",
      "        1    0.011    0.011   52.902   52.902 C:\\Users\\radiu\\AppData\\Local\\Temp\\ipykernel_11996\\1472510363.py:2(run_pipeline)\n",
      "        1    0.000    0.000   47.320   47.320 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:374(fit)\n",
      "        1    0.000    0.000   24.216   24.216 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:336(_fit)\n",
      "        1    0.000    0.000   24.215   24.215 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\joblib\\memory.py:348(__call__)\n",
      "        1    0.006    0.006   24.215   24.215 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:883(_fit_transform_one)\n",
      "        1    0.000    0.000   24.210   24.210 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2107(fit_transform)\n",
      "        1    0.345    0.345   23.978   23.978 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1342(fit_transform)\n",
      "        1    0.008    0.008   23.104   23.104 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:859(fit)\n",
      "        1    0.002    0.002   23.096   23.096 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:176(fit)\n",
      "        2    9.334    4.667   23.009   11.504 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1258(_count_vocab)\n",
      "        1   22.819   22.819   22.820   22.820 {method 'build' of 'sklearn.tree._tree.DepthFirstTreeBuilder' objects}\n",
      "  1010773    1.146    0.000   10.336    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:75(_analyze)\n",
      "  1010773    3.701    0.000    5.712    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:245(_word_ngrams)\n",
      "        1    0.005    0.005    5.571    5.571 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:449(predict)\n",
      "        1    0.000    0.000    5.503    5.503 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2139(transform)\n",
      "        1    0.001    0.001    5.447    5.447 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1410(transform)\n",
      "        1    1.264    1.264    3.862    3.862 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1203(_sort_features)\n",
      "  1010773    2.505    0.000    2.505    0.000 {method 'findall' of 're.Pattern' objects}\n",
      "        5    2.486    0.497    2.486    0.497 {built-in method builtins.sorted}\n",
      "        1    1.454    1.454    2.209    2.209 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1217(_limit_features)\n",
      "  1010773    1.160    0.000    1.160    0.000 {method 'extend' of 'array.array' objects}\n",
      "  9110097    1.027    0.000    1.027    0.000 {method 'join' of 'str' objects}\n",
      "  152/151    0.903    0.006    0.903    0.006 {built-in method numpy.asarray}\n",
      " 10120923    0.803    0.000    0.803    0.000 {method 'append' of 'list' objects}\n",
      "  1010773    0.598    0.000    0.598    0.000 {method 'extend' of 'list' objects}\n",
      "  1010773    0.386    0.000    0.519    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:213(decode)\n",
      "  1010773    0.259    0.000    0.455    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:49(_preprocess)\n",
      "  1966880    0.368    0.000    0.368    0.000 {method 'add' of 'set' objects}\n",
      "       10    0.000    0.000    0.311    0.031 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:629(check_array)\n",
      "        7    0.011    0.002    0.311    0.044 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:447(_ensure_sparse_format)\n",
      "        5    0.000    0.000    0.294    0.059 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\base.py:483(_validate_data)\n",
      "  120/119    0.185    0.002    0.290    0.002 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "        5    0.000    0.000    0.286    0.057 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1159(sort_indices)\n",
      "        3    0.278    0.093    0.278    0.093 {built-in method scipy.sparse._sparsetools.csr_sort_indices}\n",
      "        5    0.000    0.000    0.266    0.053 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:510(_mul_dispatch)\n",
      "        2    0.000    0.000    0.230    0.115 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1693(transform)\n",
      "  1010773    0.196    0.000    0.196    0.000 {method 'lower' of 'str' objects}\n",
      "  1010777    0.187    0.000    0.187    0.000 {built-in method builtins.min}\n",
      "        2    0.000    0.000    0.178    0.089 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:923(_document_frequency)\n",
      "        2    0.000    0.000    0.178    0.089 <__array_function__ internals>:177(bincount)\n",
      "  2021811    0.178    0.000    0.178    0.000 {built-in method builtins.len}\n",
      "        4    0.000    0.000    0.170    0.042 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:348(asformat)\n",
      "        1    0.003    0.003    0.169    0.169 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_csr.py:172(tocsc)\n",
      "        2    0.000    0.000    0.167    0.084 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:589(__mul__)\n",
      "        2    0.000    0.000    0.167    0.084 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:507(_mul_sparse_matrix)\n",
      "        1    0.154    0.154    0.154    0.154 {built-in method scipy.sparse._sparsetools.csr_tocsc}\n",
      "  1011483    0.133    0.000    0.133    0.000 {built-in method builtins.isinstance}\n",
      "        2    0.132    0.066    0.132    0.066 {built-in method scipy.sparse._sparsetools.csr_matmat}\n",
      "        1    0.000    0.000    0.123    0.123 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:46(__getitem__)\n",
      "        1    0.001    0.001    0.123    0.123 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_csr.py:320(_get_sliceXarray)\n",
      "        1    0.000    0.000    0.122    0.122 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:754(_minor_index_fancy)\n",
      "        2    0.113    0.057    0.113    0.057 {method 'take' of 'numpy.ndarray' objects}\n",
      "  1010773    0.110    0.000    0.110    0.000 {method 'keys' of 'dict' objects}\n",
      "        3    0.002    0.001    0.108    0.036 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:599(sum)\n",
      "        3    0.000    0.000    0.106    0.035 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:1050(sum)\n",
      "        3    0.000    0.000    0.099    0.033 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:632(__rmatmul__)\n",
      "        3    0.000    0.000    0.099    0.033 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:605(_rmul_dispatch)\n",
      "        3    0.000    0.000    0.099    0.033 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:480(_mul_vector)\n",
      "        3    0.099    0.033    0.099    0.033 {built-in method scipy.sparse._sparsetools.csc_matvec}\n",
      "        1    0.089    0.089    0.089    0.089 {built-in method scipy.sparse._sparsetools.csr_column_index2}\n",
      "  1010773    0.088    0.000    0.088    0.000 {method 'values' of 'dict' objects}\n",
      "        2    0.000    0.000    0.076    0.038 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_data.py:68(astype)\n",
      "        1    0.000    0.000    0.063    0.063 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:402(predict)\n",
      "       18    0.061    0.003    0.061    0.003 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        1    0.000    0.000    0.058    0.058 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1643(fit)\n",
      "        8    0.000    0.000    0.055    0.007 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:96(_assert_all_finite)\n",
      "        7    0.000    0.000    0.054    0.008 <__array_function__ internals>:177(sum)\n",
      "        7    0.000    0.000    0.054    0.008 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2188(sum)\n",
      "        8    0.000    0.000    0.054    0.007 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:69(_wrapreduction)\n",
      "        2    0.000    0.000    0.047    0.023 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1758(normalize)\n",
      "       44    0.044    0.001    0.044    0.001 {built-in method numpy.array}\n",
      "        1    0.000    0.000    0.044    0.044 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:389(_validate_X_predict)\n",
      "       14    0.000    0.000    0.044    0.003 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:26(__init__)\n",
      "        2    0.000    0.000    0.040    0.020 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_data.py:30(_deduped_data)\n",
      "        2    0.000    0.000    0.040    0.020 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1111(sum_duplicates)\n",
      "        3    0.000    0.000    0.037    0.012 <__array_function__ internals>:177(unique)\n",
      "        3    0.002    0.001    0.037    0.012 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:138(unique)\n",
      "       13    0.035    0.003    0.035    0.003 {method 'astype' of 'numpy.ndarray' objects}\n",
      "        2    0.035    0.017    0.035    0.017 {built-in method scipy.sparse._sparsetools.csr_matmat_maxnnz}\n",
      "        3    0.009    0.003    0.035    0.012 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:323(_unique1d)\n",
      "        1    0.033    0.033    0.033    0.033 {built-in method scipy.sparse._sparsetools.csr_column_index1}\n",
      "        2    0.029    0.014    0.029    0.014 {sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l2}\n",
      "        3    0.019    0.006    0.019    0.006 {method 'argsort' of 'numpy.ndarray' objects}\n",
      "        5    0.000    0.000    0.014    0.003 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:51(_wrapfunc)\n",
      "        1    0.014    0.014    0.014    0.014 {method 'predict' of 'sklearn.tree._tree.Tree' objects}\n",
      "        2    0.000    0.000    0.012    0.006 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1224(_with_data)\n",
      "        4    0.012    0.003    0.012    0.003 {method 'copy' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.011    0.011 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py:198(check_classification_targets)\n",
      "        2    0.000    0.000    0.011    0.005 <__array_function__ internals>:177(cumsum)\n",
      "        1    0.000    0.000    0.011    0.011 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py:221(type_of_target)\n",
      "        2    0.000    0.000    0.011    0.005 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2523(cumsum)\n",
      "        1    0.000    0.000    0.011    0.011 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py:83(unique_values)\n",
      "        2    0.011    0.005    0.011    0.005 {method 'cumsum' of 'numpy.ndarray' objects}\n",
      "        2    0.008    0.004    0.008    0.004 {method 'sort' of 'numpy.ndarray' objects}\n",
      "        1    0.008    0.008    0.008    0.008 {built-in method scipy.sparse._sparsetools.csr_sum_duplicates}\n",
      "        5    0.000    0.000    0.008    0.002 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1127(__get_sorted)\n",
      "        4    0.008    0.002    0.008    0.002 {built-in method scipy.sparse._sparsetools.csr_has_sorted_indices}\n",
      "        4    0.000    0.000    0.007    0.002 {method 'sum' of 'numpy.ndarray' objects}\n",
      "        4    0.000    0.000    0.007    0.002 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:47(_sum)\n",
      "        2    0.000    0.000    0.006    0.003 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1081(__get_has_canonical_format)\n",
      "        2    0.006    0.003    0.006    0.003 {built-in method scipy.sparse._sparsetools.csr_has_canonical_format}\n",
      "        3    0.000    0.000    0.006    0.002 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py:287(sum)\n",
      "        2    0.000    0.000    0.004    0.002 <__array_function__ internals>:177(where)\n",
      "        1    0.000    0.000    0.003    0.003 <__array_function__ internals>:177(argmax)\n",
      "        1    0.000    0.000    0.003    0.003 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1153(argmax)\n",
      "        1    0.003    0.003    0.003    0.003 {method 'argmax' of 'numpy.ndarray' objects}\n",
      "        3    0.003    0.001    0.003    0.001 {method 'flatten' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.002    0.002 <__array_function__ internals>:177(copy)\n",
      "        1    0.000    0.000    0.002    0.002 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:871(copy)\n",
      "        3    0.000    0.000    0.002    0.001 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\base.py:592(_validate_params)\n",
      "        5    0.000    0.000    0.002    0.000 <__array_function__ internals>:177(copyto)\n",
      "        1    0.000    0.000    0.001    0.001 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1207(check_random_state)\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method numpy.ascontiguousarray}\n",
      "        3    0.000    0.000    0.001    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:28(validate_parameter_constraints)\n",
      "        4    0.000    0.000    0.001    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py:150(ones)\n",
      "       35    0.001    0.000    0.001    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_sputils.py:147(get_index_dtype)\n",
      "        4    0.000    0.000    0.001    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\base.py:153(get_params)\n",
      "        1    0.000    0.000    0.001    0.001 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py:290(full)\n",
      "        4    0.000    0.000    0.001    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\base.py:122(_get_param_names)\n",
      "  191/189    0.000    0.000    0.001    0.000 {built-in method builtins.hasattr}\n",
      "        4    0.000    0.000    0.001    0.000 C:\\Users\\radiu\\anaconda3\\lib\\inspect.py:3111(signature)\n",
      "        4    0.000    0.000    0.001    0.000 C:\\Users\\radiu\\anaconda3\\lib\\inspect.py:2859(from_callable)\n",
      "        4    0.000    0.000    0.001    0.000 C:\\Users\\radiu\\anaconda3\\lib\\inspect.py:2246(_signature_from_callable)\n",
      "        5    0.000    0.000    0.001    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1322(check_is_fitted)\n",
      "       14    0.000    0.000    0.001    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:136(check_format)\n",
      "        1    0.000    0.000    0.001    0.001 C:\\Users\\radiu\\anaconda3\\lib\\contextlib.py:76(inner)\n",
      "        1    0.000    0.000    0.001    0.001 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_construct.py:73(diags)\n",
      "        4    0.000    0.000    0.001    0.000 C:\\Users\\radiu\\anaconda3\\lib\\inspect.py:2152(_signature_from_function)\n",
      "        2    0.000    0.000    0.001    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1381(<listcomp>)\n",
      "        2    0.000    0.000    0.001    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1734(idf_)\n",
      "       36    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:74(<listcomp>)\n",
      "       14    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:467(is_satisfied_by)\n",
      "    63/62    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:103(make_constraint)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:983(tocsr)\n",
      "       22    0.000    0.000    0.000    0.000 {built-in method numpy.empty}\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_csr.py:135(transpose)\n",
      "      102    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\abc.py:117(__instancecheck__)\n",
      "       72    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\getlimits.py:668(__init__)\n",
      "       11    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:446(__contains__)\n",
      "      102    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "       15    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1168(prune)\n",
      "       44    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\inspect.py:2498(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_dia.py:392(tocoo)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\contextlib.py:123(__exit__)\n",
      "       16    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\_ufunc_config.py:33(seterr)\n",
      "       75    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(can_cast)\n",
      "        1    0.000    0.000    0.000    0.000 {function SeedSequence.generate_state at 0x0000029F34DB9F70}\n",
      "       16    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_sputils.py:295(check_shape)\n",
      "        8    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:320(_num_samples)\n",
      "        8    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\_ufunc_config.py:430(__enter__)\n",
      "       20    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py:90(get_namespace)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_coo.py:372(tocsr)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:148(_validate_indices)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(argsort)\n",
      "      113    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1038(argsort)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\codeop.py:142(__call__)\n",
      "       14    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:462(_has_valid_type)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\n",
      "       28    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\_config.py:30(get_config)\n",
      "        5    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\base.py:394(_check_feature_names)\n",
      "        8    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\_ufunc_config.py:435(__exit__)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\contextlib.py:261(helper)\n",
      "       67    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:291(nnz)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\inspect.py:2781(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_dia.py:89(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\random.py:791(getrandbits)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(concatenate)\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_coo.py:127(__init__)\n",
      "    29/28    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\abc.py:121(__subclasscheck__)\n",
      "        5    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1870(_get_feature_names)\n",
      "       14    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_sputils.py:238(isshape)\n",
      "        7    0.000    0.000    0.000    0.000 {built-in method numpy.zeros}\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\contextlib.py:86(__init__)\n",
      "        6    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:226(_ascontainer)\n",
      "       43    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:749(__getattr__)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:214(_validate_steps)\n",
      "    29/28    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "       16    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\_ufunc_config.py:132(geterr)\n",
      "       44    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\enum.py:358(__call__)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py:82(_validate_names)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:522(__init__)\n",
      "        6    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_sputils.py:380(asmatrix)\n",
      "       13    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_sputils.py:211(isscalarlike)\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1385(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:173(get_params)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:78(_csc_container)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py:30(_get_params)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:172(_asindices)\n",
      "        3    0.000    0.000    0.000    0.000 {method 'max' of 'numpy.ndarray' objects}\n",
      "       16    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_data.py:20(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method nt.urandom}\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:39(_amax)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_csr.py:333(isspmatrix_csr)\n",
      "       10    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:398(parent)\n",
      "       63    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:108(getnnz)\n",
      "      149    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:119(get_shape)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:267(_unpack_index)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py:126(is_multilabel)\n",
      "       35    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\getlimits.py:679(min)\n",
      "       28    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\_config.py:22(_get_threadlocal_config)\n",
      "       25    0.000    0.000    0.000    0.000 {built-in method builtins.any}\n",
      "       48    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_sputils.py:308(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method scipy.sparse._sparsetools.coo_tocsr}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_coo.py:266(_check)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\inspect.py:494(unwrap)\n",
      "       13    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py:1878(isscalar)\n",
      "        6    0.000    0.000    0.000    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:590(_pandas_dtype_needs_early_conversion)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\base.py:136(<listcomp>)\n",
      "       46    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:1301(isspmatrix)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:249(_iter)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:530(is_satisfied_by)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_sputils.py:216(isintlike)\n",
      "       33    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:258(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:423(build_analyzer)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\base.py:742(__iter__)\n",
      "    21/18    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py:167(__array_finalize__)\n",
      "        5    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\base.py:348(_check_n_features)\n",
      "        8    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:547(__init__)\n",
      "       16    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:105(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_sputils.py:272(validateaxis)\n",
      "       16    0.000    0.000    0.000    0.000 {built-in method numpy.seterrobj}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:506(_prune_tree)\n",
      "       30    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\_lib\\_util.py:143(_prune_array)\n",
      "       66    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_data.py:23(_get_dtype)\n",
      "        8    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:555(is_satisfied_by)\n",
      "       37    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\getlimits.py:692(max)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(amax)\n",
      "        5    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(ravel)\n",
      "       99    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.vars}\n",
      "        5    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:267(_num_features)\n",
      "       10    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:571(_ensure_no_complex_data)\n",
      "       48    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\inspect.py:2830(<genexpr>)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_sputils.py:22(upcast)\n",
      "        4    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(atleast_1d)\n",
      "        7    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:708(_values)\n",
      "       61    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:226(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:398(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py:168(_asarray_with_order)\n",
      "        5    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\warnings.py:165(simplefilter)\n",
      "       28    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2703(amax)\n",
      "        5    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\inspect.py:73(isclass)\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_sputils.py:202(get_sum_dtype)\n",
      "        4    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(ndim)\n",
      "        9    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:596(dtype)\n",
      "       22    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\contextlib.py:114(__enter__)\n",
      "        3    0.000    0.000    0.000    0.000 {method 'min' of 'numpy.ndarray' objects}\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\numerictypes.py:356(issubdtype)\n",
      "       62    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:349(check_memory)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:531(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_available_if.py:25(__get__)\n",
      "        5    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\warnings.py:181(_add_filter)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method numpy.frombuffer}\n",
      "        5    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1781(ravel)\n",
      "       11    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:328(is_satisfied_by)\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:43(_amin)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:376(_compatible_boolean_index)\n",
      "       32    0.000    0.000    0.000    0.000 {built-in method numpy.geterrobj}\n",
      "        7    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2069(internal_values)\n",
      "        4    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1033(_handle_fromlist)\n",
      "       44    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\enum.py:670(__new__)\n",
      "        4    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}\n",
      "        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(empty_like)\n",
      "       11    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py:70(asarray)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:407(_check_params)\n",
      "        6    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1143(__set_sorted)\n",
      "       15    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_sputils.py:93(to_native)\n",
      "       44    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}\n",
      "       17    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py:63(__getattr__)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py:23(atleast_1d)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\joblib\\memory.py:916(__init__)\n",
      "        9    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2058(dtype)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Local\\Temp\\ipykernel_11996\\2777130815.py:7(<module>)\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1103(__set_has_canonical_format)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:353(build_tokenizer)\n",
      "       10    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:581(_check_estimator_name)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(tile)\n",
      "        5    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\warnings.py:458(__enter__)\n",
      "        8    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\inspect.py:159(isfunction)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:125(_set_self)\n",
      "       10    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py:1278(is_bool_dtype)\n",
      "        6    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\numerictypes.py:282(issubclass_)\n",
      "        5    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_dia.py:446(isspmatrix_dia)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.arange}\n",
      "      128    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\inspect.py:2548(name)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\traitlets\\traitlets.py:689(__get__)\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:307(_final_estimator)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(reshape)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\IPython\\core\\compilerop.py:180(extra_flags)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\accessor.py:178(__get__)\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:125(_unpack_tuple)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1023(is_classifier)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\inspect.py:514(_is_wrapper)\n",
      "       75    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\multiarray.py:502(can_cast)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\base.py:151(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\lib\\shape_base.py:1191(tile)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:953(_print_elapsed_time)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3152(ndim)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_construct.py:173(<listcomp>)\n",
      "        8    0.000    0.000    0.000    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
      "       15    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:262(is_satisfied_by)\n",
      "        6    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5888(__getattr__)\n",
      "        7    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:198(reshape)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\re.py:250(compile)\n",
      "       16    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:564(<genexpr>)\n",
      "       33    0.000    0.000    0.000    0.000 {built-in method _operator.index}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:361(_first_element_bool)\n",
      "       80    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\inspect.py:2560(kind)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\sparse\\accessor.py:29(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:70(<dictcomp>)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method sys.getrecursionlimit}\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_coo.py:240(getnnz)\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "       14    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:828(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\re.py:289(_compile)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py:192(is_sparse)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1979(__init__)\n",
      "       48    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_sputils.py:313(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\traitlets\\traitlets.py:651(get)\n",
      "        7    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\_ufunc_config.py:426(__init__)\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method _operator.lt}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\joblib\\memory.py:100(_store_backend_factory)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:44(check)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method from_bytes}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:73(_coo_container)\n",
      "       36    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_csr.py:231(_swap)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.hash}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:83(_csr_container)\n",
      "        5    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\warnings.py:437(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\warnings.py:477(__exit__)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:352(_maybe_bool_ndarray)\n",
      "       11    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_sputils.py:268(isdense)\n",
      "       15    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_csc.py:230(_swap)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(atleast_2d)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\sparse\\accessor.py:45(_validate)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1494(_make_int_array)\n",
      "       15    0.000    0.000    0.000    0.000 {built-in method numpy.asanyarray}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:121(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:846(__array__)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:326(build_preprocessor)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:373(get_stop_words)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383(_check_stop_words_consistency)\n",
      "        5    0.000    0.000    0.000    0.000 {method 'remove' of 'list' objects}\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_sputils.py:56(upcast_char)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_sputils.py:135(getdata)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:653(is_satisfied_by)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:319(_check_fit_params)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:313(_check_ellipsis)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3421(compare)\n",
      "        7    0.000    0.000    0.000    0.000 {built-in method _operator.ge}\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2009(_block)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2519(_cumsum_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1164(__init__)\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method builtins.callable}\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:312(_log_message)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\generic.py:45(_instancecheck)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\joblib\\memory.py:971(cache)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py:1556(get_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:1229(user_global_ns)\n",
      "        7    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:962(_check_large_sparse)\n",
      "       15    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_sputils.py:109(getdtype)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:505(_check_vocabulary)\n",
      "       15    0.000    0.000    0.000    0.000 {built-in method _warnings._filters_mutated}\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:197(_check_stop_list)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method _operator.gt}\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\inspect.py:2865(parameters)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py:81(atleast_2d)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1637(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\multiarray.py:345(where)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\generic.py:40(_check)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:603(dtypes)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:714(_major_slice)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525(_warn_for_unused_params)\n",
      "        5    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\multiarray.py:1079(copyto)\n",
      "        5    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
      "        5    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1777(_ravel_dispatcher)\n",
      "        7    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2183(_sum_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:500(dtype)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:654(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py:1433(is_extension_array_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\multiarray.py:152(concatenate)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:139(iloc)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:508(is_satisfied_by)\n",
      "        8    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:272(is_satisfied_by)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3148(_ndim_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:145(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.repr}\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\multiarray.py:891(bincount)\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:133(_unique_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:475(_validate_vocabulary)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2070(_check_params)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'intersection' of 'set' objects}\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py:19(_atleast_1d_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:320(<dictcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\contextlib.py:63(_recreate_cm)\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\multiarray.py:84(empty_like)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\joblib\\memory.py:345(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\joblib\\logger.py:67(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py:92(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1034(_argsort_dispatcher)\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py:253(_collapse)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\lib\\shape_base.py:1263(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:193(_reshape_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1149(_argmax_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2698(_amax_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:515(_validate_ngram_range)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py:77(_atleast_2d_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:867(_copy_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\numpy\\lib\\shape_base.py:1187(_tile_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_csc.py:124(tocsc)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x29f3a0c45e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Профілювання виконання пайплайну\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "# Виконання пайплайну\n",
    "valid_pred = run_pipeline(train_texts, y_train, valid_texts)\n",
    "profiler.disable()\n",
    "\n",
    "# Виведення результатів профілювання\n",
    "stats = pstats.Stats(profiler).sort_stats('cumtime')\n",
    "stats.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:42:13.060465400Z",
     "start_time": "2023-11-21T14:42:13.003466500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5903662136813695"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_result = accuracy_score(y_valid, valid_pred)\n",
    "\n",
    "my_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:2em;\">Завдання 2</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1:** Використайте триграми (3-грамні комбінації слів) у векторизаторі TF-IDF для побудови моделі класифікації. Триграми дають змогу вловити контекст, коли три слова розташовані поруч. Використайте `TfidfVectorizer(ngram_range=(3, 3))` для векторизації, а потім застосуйте логістичну регресію. Оцініть, чи триграми покращують точність проти уніграм або біграм.\n",
    "\n",
    "*Технічна примітка:* Застосуйте метод `fit_transform()` для векторизації текстів, а потім використайте `LogisticRegression()` для побудови моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2:** Використайте поєднання уніграм та біграм у TF-IDF векторизаторі для кращого охоплення локального контексту. Це допоможе врахувати не тільки поодинокі слова, але й поширені пари слів. Використайте `ngram_range=(1, 2)` у `TfidfVectorizer`, щоб включити й уніграми, і біграми, та побудуйте модель на основі отриманих ознак.\n",
    "\n",
    "*Технічна примітка:* Використайте метод `fit_transform()` для підготовки ознак, а після цього `LogisticRegression()` для навчання моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3:** Дослідіть вплив регуляризації L1 (Lasso) на модель логістичної регресії. Регуляризація L1 додає штраф за величину коефіцієнтів, тим самим надаючи змогу деяким ознакам мати нульові ваги, що призводить до менш складної моделі. Використайте `LogisticRegression(penalty='l1', solver='liblinear')` і порівняйте результати з базовою моделлю без регуляризації.\n",
    "\n",
    "*Технічна примітка:* Застосуйте параметр `C` для керування ступенем регуляризації та використайте метод `fit()` для навчання моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4:** Застосуйте регуляризацію L2 (Ridge) до моделі логістичної регресії. L2 регуляризація змушує всі коефіцієнти бути малими, але не нульовими, що дає змогу враховувати всі ознаки, але зменшує їхні ваги. Використайте `LogisticRegression(penalty='l2', solver='lbfgs')` для навчання моделі з регуляризацією.\n",
    "\n",
    "*Технічна примітка:* Використайте метод `fit()` для навчання, і порівняйте значення точності з моделлю без регуляризації.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5:** Проведіть експерименти з параметром `max_features` у TF-IDF векторизаторі, обмежуючи кількість ознак до 1000. Цей параметр дає змогу контролювати розмір простору ознак, що може прискорити процес навчання. Використайте `TfidfVectorizer(max_features=1000)` для обмеження кількості ознак та перевірте, як це впливає на якість моделі.\n",
    "\n",
    "*Технічна примітка:* Застосуйте метод `fit_transform()` для побудови матриці ознак і проаналізуйте результати за допомогою метрик точності, F1-міри та інших.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6:** Підвищіть кількість ознак у TF-IDF векторизаторі до 20,000, щоб врахувати рідкісні слова та специфічні терміни. Використайте `TfidfVectorizer(max_features=20000)` і перевірте, як збільшення кількості ознак впливає на продуктивність моделі. Проаналізуйте, чи покращуються показники моделі.\n",
    "\n",
    "*Технічна примітка:* Використайте методи `fit_transform()` і `LogisticRegression()` для підготовки та навчання моделі, після чого оцініть результати.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7:** Змініть поріг ймовірності для прийняття рішень у логістичній регресії з 0.5 на 0.7 і порівняйте результати. Це дасть можливість зменшити кількість хибнопозитивних результатів, але може збільшити кількість хибнонегативних. Для цього використайте метод `predict_proba()` і порівняйте отримані результати з різними порогами.\n",
    "\n",
    "*Технічна примітка:* Використайте `predict_proba()` для отримання ймовірностей і `np.where()` для зміни порогового значення.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8:** Проведіть експерименти з кількома порогами ймовірності (наприклад, 0.3, 0.5, 0.7) у логістичній регресії. Дослідіть, як зміна порогу впливає на показники моделі, такі як точність, чутливість і специфічність. Використайте `predict_proba()` для отримання ймовірностей і метод `precision_recall_curve()` з `sklearn.metrics` для аналізу впливу порогу.\n",
    "\n",
    "*Технічна примітка:* Застосуйте функцію `precision_recall_curve()` для побудови графіків.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9:** Застосуйте 10-кратну перехресну перевірку (k-fold cross-validation) для моделі логістичної регресії, щоб оцінити її стабільність і узагальнювальну здатність. Використайте метод `cross_val_score()` з бібліотеки `sklearn.model_selection` і порівняйте середнє значення точності та стандартне відхилення.\n",
    "\n",
    "*Технічна примітка:* Використайте параметр `cv=10` у методі `cross_val_score()` для 10-кратної перевірки.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10:** Порівняйте 5-кратну та 10-кратну перехресну перевірку для моделі логістичної регресії, щоб оцінити, який метод дає більш стабільні результати. Застосуйте метод `cross_val_score()` для обох конфігурацій та проаналізуйте отримані результати.\n",
    "\n",
    "*Технічна примітка:* Використайте параметри `cv=5` та `cv=10` у методі `cross_val_score()` і порівняйте середні значення метрик.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11:** Проаналізуйте значення коефіцієнтів моделі логістичної регресії, щоб зрозуміти, які ознаки найбільше впливають на класифікацію. Використайте атрибут `coef_`, щоб вивести вагові коефіцієнти для кожної ознаки.\n",
    "\n",
    "*Технічна примітка:* Після навчання моделі використайте атрибут `coef_` для аналізу значень коефіцієнтів.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12:** Порівняйте коефіцієнти моделі логістичної регресії для різних $n$-грам, щоб зрозуміти, як певні словосполучення впливають на класифікацію. Використайте атрибут `coef_` для отримання коефіцієнтів та побудуйте таблицю найбільш вагомих ознак.\n",
    "\n",
    "*Технічна примітка:* Застосуйте функцію `get_feature_names_out()` для отримання назв ознак та `coef_` для їхніх коефіцієнтів.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13:** Побудуйте криву навчання для моделі логістичної регресії, щоб оцінити, як продуктивність моделі змінюється залежно від кількості навчальних даних. Використайте функцію `learning_curve()` з бібліотеки `sklearn.model_selection` для створення графіка.\n",
    "\n",
    "*Технічна примітка:* Застосуйте параметри `train_sizes` для налаштування обсягів навчальних даних та `cv=5` для перехресної перевірки.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14:** Побудуйте криву валідації для логістичної регресії, щоби проаналізувати, як зміна параметра регуляризації впливає на продуктивність моделі. Використайте функцію `validation_curve()` з `sklearn.model_selection` і налаштуйте параметр `C`.\n",
    "\n",
    "*Технічна примітка:* Використайте параметр `param_name='C'` у `validation_curve()` для оцінювання впливу регуляризації.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15:** Оптимізуйте параметри логістичної регресії за допомогою `GridSearchCV`. Налаштуйте параметри регуляризації (`C`) та тип регуляризації (`penalty`) для пошуку оптимальних значень. Застосуйте метод `GridSearchCV()` для налаштування та оцінювання продуктивності.\n",
    "\n",
    "*Технічна примітка:* Використайте `param_grid` для налаштування діапазону значень параметрів і `scoring='accuracy'` для оцінювання продуктивності.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16:** Налаштуйте гіперпараметри логістичної регресії за допомогою `RandomizedSearchCV`, щоб оцінити, чи можна досягти таких самих або кращих результатів, як із `GridSearchCV`. Використайте більший діапазон значень для параметрів, щоб оптимізувати пошук.\n",
    "\n",
    "*Технічна примітка:* Використайте параметр `n_iter=100` для обмеження кількості ітерацій і параметр `random_state` для відтворюваності.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17:** Виведіть назви ознак із TF-IDF векторизатора за допомогою функції `get_feature_names_out()` і проаналізуйте, які слова або фрази мають найбільший вплив на класифікацію. Порівняйте це з ваговими коефіцієнтами моделі логістичної регресії.\n",
    "\n",
    "*Технічна примітка:* Використайте `get_feature_names_out()` для отримання ознак і поєднайте їх із коефіцієнтами за допомогою `coef_`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18:** Побудуйте графік розподілу TF-IDF ваг для кожної ознаки за допомогою бібліотеки `matplotlib` або `seaborn`. Це допоможе візуалізувати, як часто певні слова з’являються в текстах та який вплив вони мають на модель.\n",
    "\n",
    "*Технічна примітка:* Використайте метод `get_feature_names_out()` для отримання ознак і `coef_` для отримання ваг, а потім побудуйте графік за допомогою `seaborn.barplot()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19:** Проаналізуйте топ-20 найважливіших ознак, які мають найвищі TF-IDF ваги в моделі логістичної регресії. Використайте функцію `argsort()` для сортування коефіцієнтів і отримання найвпливовіших слів.\n",
    "\n",
    "*Технічна примітка:* Застосуйте `argsort()` до `coef_`, щоб впорядкувати коефіцієнти за величиною та отримати топ-20 ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20:** Проаналізуйте слова, що мають найменші TF-IDF значення, та дослідіть їхній вплив на модель. Розгляньте можливість вилучення таких ознак із моделі. Використайте `get_feature_names_out()` для отримання назв ознак та порівняйте їх із коефіцієнтами моделі.\n",
    "\n",
    "*Технічна примітка:* Використайте функцію `argsort()` для пошуку найменших коефіцієнтів і їхнього аналізу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-4.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fefd0178f43ce832031653be70f0a0e47f62cf4c"
   },
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">4.3. Інтерпретація та порівняння моделей</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:41:17.605160200Z",
     "start_time": "2023-11-21T14:41:17.590161300Z"
    },
    "_uuid": "247a13fd3ae4d5c015c0ca0489a9a95d72ad7e9f"
   },
   "outputs": [],
   "source": [
    "# Довідково: Нижче подамо матрицю невідповідностей\n",
    "def plot_confusion_matrix(\n",
    "    actual,\n",
    "    predicted,\n",
    "    classes,\n",
    "    normalize=False,\n",
    "    title=\"Confusion matrix\",\n",
    "    figsize=(7, 7),\n",
    "    cmap=plt.cm.Blues,\n",
    "    path_to_save_fig=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm = confusion_matrix(actual, predicted).T\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"Predicted label\")\n",
    "    plt.xlabel(\"True label\")\n",
    "\n",
    "    if path_to_save_fig:\n",
    "        plt.savefig(path_to_save_fig, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:2em;\">Завдання 3</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1:** Побудуйте криві навчання та валідування для моделі логістичної регресії, оцінюючи різницю між цими кривими на різних етапах навчання. Використайте функції `learning_curve()` з бібліотеки `sklearn.model_selection` та порівняйте отримані результати. Додатково побудуйте confusion matrix для оцінювання точності моделі, використовуючи самописну функцію `plot_confusion_matrix()`.\n",
    "\n",
    "*Технічна примітка:* Використайте параметр `train_sizes` для поступового збільшення обсягу даних та оцініть можливе перенавчання чи недонавчання.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2:** Порівняйте криві навчання та валідування для логістичної регресії на основі використання різних значень параметра регуляризації (наприклад, `C`). Візуалізуйте результати за допомогою `validation_curve()` з `sklearn.model_selection` та побудуйте confusion matrix для аналізу результатів, використовуючи самописну функцію `plot_confusion_matrix()`.\n",
    "\n",
    "*Технічна примітка:* Використайте діапазон значень параметра `C` у `validation_curve()` для виявлення найкращого балансу між перенавчанням та недонавчанням.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3:** Обчисліть та інтерпретуйте співвідношення шансів (odds ratios) для найбільш вагомих ознак у моделі логістичної регресії. Співвідношення шансів покаже, наскільки кожна ознака підвищує або знижує ймовірність сарказму. Побудуйте confusion matrix для обчислення точності моделі, використовуючи самописну функцію `plot_confusion_matrix()`.\n",
    "\n",
    "*Технічна примітка:* Використайте атрибут `coef_` для отримання коефіцієнтів логістичної регресії та обчисліть odds ratios за допомогою функції `np.exp()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4:** Порівняйте співвідношення шансів для декількох важливих ознак у моделі логістичної регресії. Проаналізуйте, які ознаки найбільш впливають на класифікацію сарказму. Побудуйте confusion matrix для візуального подання помилок моделі, використовуючи самописну функцію `plot_confusion_matrix()`.\n",
    "\n",
    "*Технічна примітка:* Використайте атрибут `coef_` для отримання коефіцієнтів і застосуйте формулу для обчислення odds ratios `exp(coef)`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5:** Виконайте аналіз випадків, коли модель логістичної регресії дала неправильні прогнози. Використайте confusion matrix для виявлення таких випадків та візуалізуйте їх. На основі цього аналізу запропонуйте стратегії покращення моделі.\n",
    "\n",
    "*Технічна примітка:* Використайте метод `predict()` для прогнозів та `plot_confusion_matrix()` для побудови матриці.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6:** Проаналізуйте помилки класифікації, що зроблені моделлю логістичної регресії, та побудуйте confusion matrix для візуалізації неправильно класифікованих випадків через самописну функцію `plot_confusion_matrix()`. Використайте цей аналіз для визначення проблемних ознак або класів та запропонуйте стратегії для покращення моделі.\n",
    "\n",
    "*Технічна примітка:* Використайте `classification_report()` з `sklearn.metrics` для додаткового статистичного оцінювання моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7:** Побудуйте модель дерева рішень за тими ж даними, що і для логістичної регресії, та порівняйте їхні результати. Для обох моделей побудуйте confusion matrix і проаналізуйте відмінності між результатами, використовуючи самописну функцію `plot_confusion_matrix()`.\n",
    "\n",
    "*Технічна примітка:* Використайте `DecisionTreeClassifier` з `sklearn.tree`, а для логістичної регресії – `LogisticRegression`. Порівняйте precision, recall та F1-score.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8:** Порівняйте результативність моделі дерева рішень із логістичною регресією на основі confusion matrix. Проаналізуйте переваги та недоліки кожної моделі з огляду на їхні здатності класифікувати сарказм у текстах.\n",
    "\n",
    "*Технічна примітка:* Використайте функцію `plot_confusion_matrix()` для кожної моделі та `GridSearchCV` для налаштування параметрів дерева рішень.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9:** Побудуйте графіки часткової залежності (PDP) для п’яти найважливіших ознак у моделі логістичної регресії, щоб оцінити, як зміна цих ознак впливає на ймовірність сарказму. Додатково побудуйте confusion matrix для оцінювання загальної продуктивності моделі, використовуючи самописну функцію `plot_confusion_matrix()`.\n",
    "\n",
    "*Технічна примітка:* Використайте `plot_partial_dependence()` з `sklearn.inspection` для візуалізації часткової залежності.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10:** Створіть графіки часткової залежності для кількох найважливіших ознак у моделі логістичної регресії, щоб оцінити їхній вплив на класифікацію сарказму. Побудуйте confusion matrix для оцінювання точності моделі, використовуючи самописну функцію `plot_confusion_matrix()`.\n",
    "\n",
    "*Технічна примітка:* Використайте параметр `features` у `plot_partial_dependence()` для вибору ключових ознак та візуалізації їхнього впливу.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11:** Оцініть модель логістичної регресії за допомогою метрик точності (accuracy), влучності (precision), повноти (recall) та F1-score. Для більш детального оцінювання побудуйте confusion matrix і проаналізуйте її, використовуючи самописну функцію `plot_confusion_matrix()`.\n",
    "\n",
    "*Технічна примітка:* Використайте функції `accuracy_score()`, `precision_score()`, `recall_score()` та `f1_score()` з `sklearn.metrics` для обчислення метрик.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12:** Застосуйте кілька статистичних метрик, як от точність (accuracy), влучність (precision), повнота (recall) та F1-score, для оцінювання якості моделі логістичної регресії. Для візуалізації помилок скористайтесь confusion matrix, використовуючи самописну функцію `plot_confusion_matrix()`.\n",
    "\n",
    "*Технічна примітка:* Використайте `classification_report()` для виведення всіх ключових метрик в одному місці та побудуйте на її основі confusion matrix.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13:** Проаналізуйте помилки моделі логістичної регресії, що були класифіковані неправильно. Використайте confusion matrix для ідентифікації цих випадків і порівняйте з правильно класифікованими.\n",
    "\n",
    "*Технічна примітка:* Використайте функцію `plot_confusion_matrix()` для побудови матриці та знайдіть основні помилки моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14:** Проведіть детальний аналіз випадків, коли модель логістичної регресії дала хибні прогнози. Візуалізуйте ці випадки за допомогою confusion matrix та проаналізуйте можливі причини помилок, використовуючи самописну функцію `plot_confusion_matrix()`.\n",
    "\n",
    "*Технічна примітка:* Використайте метрики `precision`, `recall` та `f1_score()` для оцінювання продуктивності на окремих класах.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15:** Побудуйте та проаналізуйте криві навчання та валідування для моделі логістичної регресії, щоб оцінити її можливе перенавчання чи недонавчання. Використайте confusion matrix для виявлення класів, з якими модель найгірше справляється.\n",
    "\n",
    "*Технічна примітка:* Використайте функції `learning_curve()` і `plot_confusion_matrix()` для візуалізації процесу навчання та його результатів.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16:** Застосуйте криві навчання та валідування до моделі логістичної регресії для оцінювання її стабільності. Використайте confusion matrix для обчислення точності моделі в різних класах на основі самописної функції `plot_confusion_matrix()`.\n",
    "\n",
    "*Технічна примітка:* Параметр `cv=5` у `learning_curve()` допоможе побудувати криві для кращого оцінювання моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17:** Проаналізуйте помилки класифікації в моделі логістичної регресії та візуалізуйте їх за допомогою confusion matrix. На основі аналізу запропонуйте стратегії для покращення точності класифікації та переобчисліть модель.\n",
    "\n",
    "*Технічна примітка:* Використайте `GridSearchCV` для налаштування гіперпараметрів та повторно оцініть результати за допомогою `plot_confusion_matrix()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18:** Здійсніть детальний візуальний аналіз помилок моделі логістичної регресії, побудувавши confusion matrix на основі самописної функції `plot_confusion_matrix()`. Запропонуйте вдосконалення моделі за проведеним аналізом, а потім переобчисліть модель для покращення її точності.\n",
    "\n",
    "*Технічна примітка:* Використайте метод `RandomizedSearchCV` для пошуку оптимальних гіперпараметрів.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19:** Створіть модель лінійної регресії, перетворивши цільову змінну для того, щоб вона стала кількісною. Побудуйте confusion matrix для порівняння результатів лінійної та логістичної регресій, використовуючи самописну функцію `plot_confusion_matrix()`. Оцініть, чому лінійна регресія може бути менш придатною для цього завдання.\n",
    "\n",
    "*Технічна примітка:* Використайте функцію `LinearRegression()` з `sklearn.linear_model` та порівняйте результати з логістичною регресією.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20:** Змініть цільову змінну для моделі лінійної регресії та порівняйте її результати з логістичною регресією. Візуалізуйте помилки моделі за допомогою confusion matrix та зробіть висновки щодо доцільності використання лінійної регресії для класифікаційних задач.\n",
    "\n",
    "*Технічна примітка:* Використайте `LinearRegression()` для побудови моделі та `plot_confusion_matrix()` для порівняння результатів."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-4.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5648f6ad7a14ef3a582909f7c0c72c4fc80204aa"
   },
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">4.4. Вдосконалення моделі логістичної регресії</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:2em;\">Завдання 4</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1:** Розробіть два окремі TF-IDF векторизатори: один для слів, які є унікальними для кожного коментаря (що можуть вказувати на індивідуальний стиль автора), і другий – для загальних слів, які часто трапляються у всіх текстах. Унікальні слова можуть мати значення для виявлення сарказму, оскільки вони можуть відображати специфічні нюанси мови, тоді як загальні слова забезпечують контекст. Поєднайте ці два набори ознак за допомогою `FeatureUnion` для побудови загального набору ознак для моделі логістичної регресії та порівняйте результативність цієї моделі з базовою, використовуючи метрики precison, recall та F1-score.\n",
    "\n",
    "*Технічна примітка:* Використайте два векторизатори `TfidfVectorizer()` з різними параметрами, наприклад, для уникнення загальних слів у одному векторизаторі та для їхнього включення в іншому (`max_df` або `min_df` для фільтрації). Для об’єднання ознак скористайтеся `FeatureUnion([('tfidf_unique', tfidf1), ('tfidf_common', tfidf2)])`. Для навчання моделі застосуйте `fit_transform()` для векторизації та порівняйте результативність за допомогою `classification_report()` з бібліотеки `sklearn.metrics`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2:** Додайте ознаки, що відображають частоту використання великих літер (CAPS LOCK) у коментарях, оскільки саркастичні висловлювання часто супроводжуються підвищеним емоційним забарвленням, яке виражається через капіталізовані слова або фрази. Об’єднайте ці нові ознаки із TF-IDF векторизованими текстами та побудуйте модель логістичної регресії. Перевірте результативність нової моделі проти попередньої, використовуючи precision, recall та F1-score.\n",
    "\n",
    "*Технічна примітка:* Використайте регулярний вираз `re.findall(r'\\b[A-Z]+\\b', text)` для обчислення кількості слів, що написані великими літерами в кожному коментарі. Цю інформацію додайте як нову ознаку до набору даних. Для побудови моделі скористайтесь `Pipeline()`, поєднавши TF-IDF векторизатор та логістичну регресію, і використайте `fit_transform()` для використання нових ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3:** Інтегруйте аналіз тональності коментарів як нові ознаки до моделі логістичної регресії, щоб врахувати емоційне забарвлення тексту. Оцінювання полярності (позитивна, негативна або нейтральна) може бути особливо корисною під час класифікації сарказму, де емоційні індикатори часто вказують на приховані сенси. Використайте `TextBlob` для аналізу тональності текстів і додайте отримані значення як додаткові ознаки до набору TF-IDF ознак. Порівняйте результати з базовою моделлю та оцініть, як додаткові ознаки вплинули на загальну точність класифікації.\n",
    "\n",
    "*Технічна примітка:* Використайте метод `TextBlob(text).sentiment.polarity` для оцінювання тональності кожного коментаря. Значення полярності змінюється від -1 (негативна тональність) до 1 (позитивна тональність). Додайте отримані оцінки як нові ознаки до DataFrame з основними TF-IDF ознаками та використайте `Pipeline()` для побудови та навчання моделі логістичної регресії.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4:** Побудуйте дві окремі моделі логістичної регресії для коротких і довгих коментарів, оскільки довжина тексту може впливати на точність виявлення сарказму. Короткі коментарі можуть містити більше концентрованого сарказму, тоді як у довших коментарях сарказм може бути замаскований складнішими виразами. Використайте середнє арифметичне ймовірностей або метод голосування (majority voting) для об’єднання результатів двох моделей і порівняйте їх із початковою моделлю. Оцініть, чи такий підхід покращує якість класифікації.\n",
    "\n",
    "*Технічна примітка:* Використайте поріг, наприклад, 50 символів, для поділу коментарів на короткі та довгі. Навчіть дві окремі моделі логістичної регресії для кожної категорії. Для об’єднання результатів використайте `VotingClassifier()` з параметром `voting='soft'`, щоб об’єднати ймовірності з обох моделей, або параметр `voting='hard'`, якщо ви використовуєте majority voting. Порівняйте результати з базовою моделлю, використовуючи precision, recall та F1-score.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5:** Реалізуйте підхід, за яким треба додати частоти вживання певних ключових слів або словосполучень, які часто асоціюються із сарказмом (наприклад, «звісно», «безумовно», «о, так»). Ці ознаки можуть стати вагомими індикаторами сарказму в тексті, оскільки подібні фрази часто використовуються іронічно. Інтегруйте отримані частоти як нові ознаки в модель поряд із TF-IDF векторизованими даними. Оцініть, чи підвищується prevision, recall, та F1-score після додавання цих специфічних ознак.\n",
    "\n",
    "*Технічна примітка:* Створіть окремі ознаки для обчислення кількості ключових слів у тексті за допомогою методу `str.count()`. Додайте ці ознаки до DataFrame з основними ознаками TF-IDF, після чого використайте `Pipeline()` для побудови та навчання моделі логістичної регресії.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6:** Розширте модель логістичної регресії, додавши ознаки, що подають часові метадані коментарів, як от день тижня, час публікації або частоту постів упродовж доби. Це може допомогти виявити тенденції, коли сарказм частіше вживається в певні періоди (наприклад, вечорами або на вихідних). Інтегруйте ці ознаки разом із TF-IDF векторизованими даними та порівняйте результативність моделі з попередньою версією.\n",
    "\n",
    "*Технічна примітка:* Використайте метод `pd.to_datetime()` для перетворення дати публікації на часові ознаки, як от `hour`, `dayofweek`, або `month`. Ці ознаки додайте до DataFrame та інтегруйте з векторизованими текстовими даними.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7:** Замість традиційного підходу TF-IDF для векторизації тексту, спробуйте застосувати метод Word2Vec для векторизації слів, щоб захопити семантичний контекст і взаємозв’язок між словами. Word2Vec здатний враховувати латентні значення слів, що дає змогу краще розпізнавати сарказм у текстах. Побудуйте модель логістичної регресії на основі середнього значення векторів для кожного тексту та порівняйте її з моделлю, що використовує TF-IDF.\n",
    "\n",
    "*Технічна примітка:* Використайте бібліотеку `gensim.models.Word2Vec` для навчання векторної моделі на текстах. Для кожного тексту обчисліть середнє значення векторів слів та використайте ці вектори як ознаки для логістичної регресії. Порівняйте результативність моделі за допомогою метрик precision, recall та F1-score.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8:** Використайте підхід до збільшення даних (data augmentation), щоб розширити набір даних, додаючи варіації оригінальних коментарів через заміну слів синонімами, перестановку частин речень або додавання нових слів. Це допоможе моделі навчитися краще розпізнавати сарказм за допомогою ширшого набору можливих текстових структур. Порівняйте точність моделі після додавання нових згенерованих даних із початковою моделлю, оцінюючи, чи збільшення даних покращило результати.\n",
    "\n",
    "*Технічна примітка:* Використайте бібліотеку `nlaug` або `textaugment` для автоматичного збільшення даних, генеруючи додаткові варіанти коментарів. Інтегруйте нові коментарі до навчального набору даних і проведіть повторне навчання моделі логістичної регресії.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9:** Додайте ознаки, що подають розподіл частоти використання символів пунктуації (зокрема, знаків питання, оклику або лапок) у текстах. Пунктуація часто використовується для передачі сарказму, тому її аналіз може дати додаткові індикатори для класифікації. Інтегруйте ці ознаки разом із TF-IDF векторизованими даними та порівняйте, чи підвищується точність та інші метрики моделі.\n",
    "\n",
    "*Технічна примітка:* Використайте метод `str.count()` для обчислення кількості частоти певних символів у текстах (наприклад, знаків питання `?`, оклику `!`, або лапок `\"`). Додайте ці ознаки до DataFrame та використайте їх для навчання моделі логістичної регресії.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10:** Інтегруйте аналіз граматичних помилок у текстах як ознаку для покращення класифікації сарказму. Часто саркастичні коментарі містять навмисні або ненавмисні граматичні помилки, що може бути корисним для моделі. Додайте кількість граматичних помилок як додаткову ознаку та порівняйте результативність моделі до й після її додавання.\n",
    "\n",
    "*Технічна примітка:* Використайте бібліотеку `language_tool_python` для автоматичного аналізу текстів на наявність граматичних помилок. Обчисліть кількість помилок у кожному тексті та додайте ці дані як нову ознаку до набору TF-IDF ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11:** Додайте до моделі логістичної регресії ознаки, що відображають стиль написання коментарів (формальний або неформальний стиль). Це може включати частоту вживання скорочень (наприклад, «щас» замість «зараз») та сленгових виразів (наприклад, «лол», «кек», «збс» тощо). Такі ознаки можуть вказувати на неформальний стиль, який іноді пов’язаний із сарказмом. Обчисліть кількість цих елементів для кожного коментаря, щоб модель могла краще розрізняти різні стилі мовлення.\n",
    "\n",
    "*Технічна примітка:* Створіть словники неформальних виразів, скорочень та сленгу, використовуючи, наприклад, API `UrbanDictionary` або створіть вручну словники для формального/неформального стилю. Підраховуйте частоту вживання цих елементів у кожному коментарі за допомогою методу `str.count()` та інтегруйте ці ознаки у DataFrame з основними TF-IDF ознаками.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12:** Додайте ознаки, що відображають риторику заперечення в коментарях (наприклад, частоту вживання заперечних конструкцій або слів типу «не», «ніколи», «жоден»). Сарказм часто використовується для підсилення заперечних висловлювань, де мовник іронічно відкидає певні факти або ситуації. Такі риторичні конструкції можуть бути корисними для розпізнавання саркастичних висловлювань. Інтегруйте ці ознаки в модель логістичної регресії та порівняйте її результати з базовою моделлю.\n",
    "\n",
    "*Технічна примітка:* Використайте регулярні вирази для обчислення кількості заперечень у кожному коментарі, наприклад, `re.findall(r'\\b(не|ніколи|жоден|ані)\\b', text)`. Ці значення можуть бути збережені як нові ознаки в DataFrame. Об’єднайте їх із TF-IDF ознаками та побудуйте модель логістичної регресії за допомогою `Pipeline()`. Оцініть результативність нової моделі, порівнюючи precision, recall, F1-score та accuracy з базовою версією моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13:** Побудуйте ансамблеву модель, що поєднує логістичну регресію та дерева рішень за допомогою методики стакінгу (stacking). Метод стакінгу дає змогу використовувати результати кількох моделей для покращення прогнозування внаслідок навчання «метамоделі», яка робить фінальне рішення на основі прогнозів базових моделей. Порівняйте результати цієї моделі з початковою логістичною регресією за допомогою метрик accuracy, precision, recall і F1-score.\n",
    "\n",
    "*Технічна примітка:* Використайте `StackingClassifier()` із `sklearn.ensemble`, поєднуючи логістичну регресію та дерево рішень, або інші базові моделі, як-от SVM або Naive Bayes. Використайте `LogisticRegression` як метамодель, яка приймає рішення на основі виходів базових моделей. Оцініть точність та інші метрики з допомогою `cross_val_score()` або `classification_report()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14:** Використайте метод ансамблю підсумовування (bagging) на основі моделі логістичної регресії для покращення її стабільності та точності. Bagging допомагає зменшити варіативність і покращити стійкість моделі до вибіркового шуму, генеруючи кілька підвибірок даних для навчання кожної моделі в ансамблі. Порівняйте результативність моделі bagging з початковою логістичною регресією.\n",
    "\n",
    "*Технічна примітка:* Використайте `BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=50, random_state=42)` з бібліотеки `sklearn.ensemble` для побудови ансамблю моделей. Порівняйте результати за допомогою метрик точності та F1-score за валідаційним набором даних, використовуючи метод `cross_val_score()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15:** Спробуйте застосувати кластеризацію для попереднього оброблення текстів, щоб групувати схожі коментарі перед їхньою векторизацією. Це може допомогти виявити патерни сарказму у великих текстових масивах і зменшити шум у даних. Додайте до моделі номер кластера як додаткову ознаку для кожного коментаря і порівняйте результативність цієї моделі з початковою логістичною регресією.\n",
    "\n",
    "*Технічна примітка:* Використайте алгоритм `KMeans` із бібліотеки `sklearn.cluster` для кластеризації текстів на основі їхніх векторизованих відображень (наприклад, TF-IDF). Додайте номер кластера як нову ознаку до кожного коментаря та інтегруйте це в модель логістичної регресії. Оцініть результативність моделі за допомогою метрик precision, recall і F1-score, використовуючи метод `classification_report()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16:** Розширте модель логістичної регресії, додавши ознаки на основі синтаксичного аналізу текстів. Ці ознаки можуть включати частоту вживання різних частин мови, таких як дієслова, прикметники, прислівники та сполучники, які можуть вказувати на стилістичні або емоційні особливості сарказму. Синтаксичні структури можуть бути корисними для виявлення тонких натяків, властивих саркастичним висловлюванням. Порівняйте результативність цієї моделі з базовою.\n",
    "\n",
    "*Технічна примітка:* Використайте бібліотеку `spacy` для синтаксичного аналізу. За допомогою атрибута `pos_` можна визначити частини мови кожного слова в тексті. Обчисліть кількість кожної частини мови та додайте ці дані як нові ознаки до DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17:** Проаналізуйте контекст дискусій, враховуючи послідовність кількох коментарів, що йдуть підряд. Для сарказму важливим може бути контекст, у якому даний коментар опублікований, наприклад, якщо це відповідь на попередній коментар або репліка в межах певної теми. Додайте інформацію про відносне розташування коментаря в діалозі, його відповідність попереднім реплікам або реакцію на них, і порівняйте результати моделі.\n",
    "\n",
    "*Технічна примітка:* Зберігайте попередні коментарі в межах одного діалогу та їхній зв’язок із поточним як нові ознаки (наприклад, `prev_comment_similarity`). Використайте текстову подібність, застосовуючи методи на основі векторів, як от cosine similarity з `sklearn.metrics.pairwise`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18:** Додайте до моделі логістичної регресії ознаки, що відображають стиль написання тексту. Це може включати довжину речень, кількість складних синтаксичних структур або співвідношення довгих та коротких слів. Такі ознаки можуть бути індикаторами формальності або стилю, який іноді пов’язаний із сарказмом. Порівняйте результативність цієї моделі з базовою, оцінюючи такі метрики, як precision, recall та F1-score.\n",
    "\n",
    "*Технічна примітка:* Використайте бібліотеку `spacy` для аналізу синтаксичних дерев тексту. Атрибут `sent_len` може використовуватись для обчислення кількості слів у реченнях, а `dependency_tree` – для оцінювання складності речень. Додайте ці ознаки до DataFrame і проаналізуйте їхній вплив на модель.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19:** Використайте додаткові ознаки, що ґрунтуються на тональності кожного окремого слова в тексті. Аналіз на рівні слів дасть можливість оцінювати емоційне забарвлення кожного слова, що може допомогти краще ідентифікувати сарказм, оскільки саркастичні висловлювання часто містять контрастні емоційні індикатори. Додайте ці оцінки як нові ознаки та порівняйте результативність моделі із базовою.\n",
    "\n",
    "*Технічна примітка:* Використайте бібліотеку `VADER` з пакету `nltk` для визначення тональності на рівні кожного слова (positive, negative, neutral). Збережіть ці значення як нові ознаки та додайте їх до DataFrame для навчання моделі логістичної регресії.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20:** Реалізуйте вибіркове оброблення текстів, вибираючи лише певні типи речень, які найчастіше містять сарказм. Наприклад, зверніть увагу на речення, що починаються зі словосполучень типу «Ну, звісно», «О, точно», «Якби ж то», які часто використовуються для передачі іронії чи сарказму. Інтегруйте ці нові ознаки в модель і порівняйте її з базовою.\n",
    "\n",
    "*Технічна примітка:* Використайте регулярні вирази (`re`) для знаходження ключових словосполучень на початку речень (наприклад, `r'^\\b(Ну, звісно|О, точно)\\b'`) і збережіть ці речення як окремі ознаки для кожного тексту. Додайте ці ознаки до основних даних для навчання моделі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-4.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">4.5. Практичне застосування результатів інтелектуального аналізу даних</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:2em;\">Завдання 5</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1:** Застосуйте модель логістичної регресії до класифікації саркастичних і не саркастичних коментарів із набору даних *train-balanced-sarcasm.csv*, використовуючи всі ознаки, що були створені на попередніх етапах (емоційне забарвлення, заперечення, соціальні сигнали, частоти великих літер тощо). Проведіть фінальне навчання на цьому збалансованому наборі та оцініть результативність моделі на тестовому наборі даних. Порівняйте результати з базовою моделлю, що використовувала лише TF-IDF ознаки.\n",
    "\n",
    "*Технічна примітка:* Використайте бібліотеку `scikit-learn` для розділення даних за допомогою `train_test_split()`. Навчіть модель за допомогою `LogisticRegression()` із налаштуванням регуляризації `C` для оптимізації. Порівняйте результати за допомогою `classification_report()` для метрик точності, precision, recall та F1-score.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2:** Проведіть аналіз результативності моделі логістичної регресії за незбалансованим набором даних, використовуючи всі створені ознаки (емоції, риторика, частота вживання великих літер). Проаналізуйте, як модель поводиться у випадках із великою кількістю негативних прикладів сарказму або недостатньою кількістю позитивних. Оцініть, чи потрібно застосовувати підходи до підвищення ваги позитивних класів для покращення результатів.\n",
    "\n",
    "*Технічна примітка:* Використайте параметр `class_weight='balanced'` у моделі `LogisticRegression()` для автоматичного підвищення ваг класів. Додатково можете використати методи підвищення вибірки (oversampling) за допомогою `SMOTE` з бібліотеки `imblearn`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3:** Створіть функцію для автоматичного прогнозування сарказму в нових коментарях, використовуючи модель логістичної регресії, навченої за набором даних *train-balanced-sarcasm.csv*. Додайте можливість прогнозування ймовірності для кожного коментаря та пояснення, які ознаки найбільше вплинули на класифікацію. Проаналізуйте важливість ознак для кожного передбаченого класу (сарказм/не сарказм).\n",
    "\n",
    "*Технічна примітка:* Використайте метод `predict_proba()` з бібліотеки `scikit-learn` для отримання ймовірностей для кожного класу. Для пояснення важливості ознак використайте бібліотеки `LIME` або `SHAP`, щоб визначити, які саме ознаки найбільше впливають на результат.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4:** Застосуйте модель логістичної регресії до аналізу довгих і коротких коментарів у наборі *train-balanced-sarcasm.csv*. Проведіть аналіз, чи сарказм частіше трапляється в коротких коментарях або в довших текстах. Побудуйте дві окремі моделі для коротких та довгих коментарів і порівняйте їхні результати за точністю та F1-score.\n",
    "\n",
    "*Технічна примітка:* Використайте функцію для поділу коментарів за довжиною (наприклад, коментарі, коротші за 50 символів – короткі, довші – довгі). Навчіть дві окремі моделі за допомогою `LogisticRegression()` для кожної категорії. Порівняйте результативність за допомогою метрик, таких як precision, recall та F1-score.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5:** Створіть модель, яка прогнозує наявність сарказму залежно від дня тижня або часу публікації коментарів, використовуючи метадані з *train-balanced-sarcasm.csv*. Проведіть аналіз, чи є залежність між частотою саркастичних коментарів та певними днями тижня (наприклад, чи частіше сарказм трапляється в п’ятницю ввечері). Використайте цю модель для прогнозування сарказму на основі тимчасових даних.\n",
    "\n",
    "*Технічна примітка:* Використайте функцію `pd.to_datetime()` для перетворення часових міток у дні тижня та години. Додайте ці ознаки до основного набору TF-IDF ознак і побудуйте модель за допомогою `LogisticRegression()`. Проведіть аналіз точності моделі для кожного дня тижня, використовуючи `cross_val_score()` для отримання середніх показників результативності.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6:** Застосуйте модель логістичної регресії до класифікації саркастичних коментарів на реальних даних із набору *train-balanced-sarcasm.csv*, включивши всі ознаки, розроблені на попередніх етапах (емоційне забарвлення, заперечення, соціальні сигнали, частота великих літер, риторичні конструкції). Проведіть фінальне тестування моделі на раніше невидимих даних і порівняйте результати з базовою моделлю, яка використовувала лише TF-IDF ознаки. Оцініть, наскільки ці нові ознаки підвищили точність класифікації сарказму.\n",
    "\n",
    "*Технічна примітка:* Використайте `LogisticRegression()` з параметром `C` для налаштування регуляризації. Для оброблення та об’єднання всіх ознак використайте клас `FeatureUnion` з бібліотеки `sklearn`. Порівняйте результати за допомогою метрик `precision`, `recall`, `F1-score`, використовуючи `classification_report()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7:** Проведіть експеримент із використанням моделі логістичної регресії на незбалансованому наборі даних із *train-balanced-sarcasm.csv*. Проаналізуйте вплив незбалансованості даних на результативність моделі, особливо в ситуаціях, де сарказм є менш представленим класом. Використайте метод підвищення ваг класів або збільшення кількості позитивних прикладів і порівняйте результати з попередніми.\n",
    "\n",
    "*Технічна примітка:* Використайте параметр `class_weight='balanced'` у `LogisticRegression()` для автоматичного коригування ваг класів. Для збалансування набору даних також можете використати `SMOTE` з бібліотеки `imblearn`, що дасть змогу створити нові приклади позитивних класів.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8:** Розробіть функцію для класифікації нових коментарів як саркастичні чи ні, використовуючи модель логістичної регресії, навченої на *train-balanced-sarcasm.csv*. Додайте можливість прогнозування ймовірності кожного класу (сарказм/не сарказм) та збережіть результат разом з оцінкою ймовірності для подальшого аналізу точності моделі на нових даних.\n",
    "\n",
    "*Технічна примітка:* Використайте функцію `predict_proba()` для виведення ймовірностей сарказму в коментарях. Збережіть результати прогнозування у DataFrame для подальшого аналізу та валідації точності моделі за новими метриками, такими як `precision`, `recall`, `F1-score`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9:** Застосуйте модель логістичної регресії до аналізу темпоральної динаміки сарказму в коментарях. Використайте часові мітки з набору *train-balanced-sarcasm.csv*, щоб дослідити, чи є взаємозв’язок між сарказмом та часом публікації коментарів (наприклад, коментарі, опубліковані у вечірній час або в певні дні тижня). Порівняйте результати моделі, коли вона враховує або ігнорує часові ознаки.\n",
    "\n",
    "*Технічна примітка:* Використайте `pd.to_datetime()` для перетворення часових міток у дні тижня або години. Додайте ці ознаки до загального набору ознак та використайте `LogisticRegression()` для оцінювання результативності моделі із часовими факторами та без них.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10:** Проведіть аналіз впливу довжини коментарів на результативність моделі логістичної регресії. Поділіть коментарі з *train-balanced-sarcasm.csv* на короткі та довгі (за заданим порогом, наприклад, 50 символів) і побудуйте окремі моделі для кожної категорії. Оцініть, чи сарказм частіше трапляється в коротких або довгих коментарях, і порівняйте результативність моделі для обох категорій.\n",
    "\n",
    "*Технічна примітка:* Використайте метод для поділу коментарів за їхньою довжиною (наприклад, функцію `len()`). Побудуйте окремі моделі за допомогою `LogisticRegression()` для кожної групи та порівняйте результати за допомогою метрик точності (`accuracy`), precision, recall і F1-score.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11:** Застосуйте модель логістичної регресії до прогнозування сарказму в коментарях, з огляду на всі створені ознаки (емоційне забарвлення, заперечення, риторичні конструкції тощо). Проведіть фінальне тестування моделі, використовуючи крос-валідацію, та оцініть її точність на основі метрик точності, precision, recall та F1-score. Порівняйте результати цієї моделі з результатами базової моделі, що використовувала лише TF-IDF ознаки.\n",
    "\n",
    "*Технічна примітка:* Використайте бібліотеку `scikit-learn`, зокрема клас `LogisticRegression()` для навчання моделі. для оцінювання результативності використайте функцію `cross_val_score()` з параметром `cv=10` для крос-валідації. Для порівняння результатів використайте `classification_report()` з метриками `accuracy`, `precision`, `recall` та `F1-score`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12:** Створіть функцію для автоматичного прогнозування сарказму на нових даних, що не входять у тренувальний набір. Зробіть прогнозування та оцініть, як модель працює з новими даними. Додатково додайте аналіз важливості ознак, що найбільше впливають на результат, щоби покращити інтерпретацію моделі.\n",
    "\n",
    "*Технічна примітка:* Використайте бібліотеку `SHAP` або `LIME` для інтерпретації впливу кожної ознаки на результат класифікації. Використайте методи векторизації та попередньо створені ознаки для аналізу нових даних для подальшого прогнозування моделі логістичної регресії.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13:** Проведіть аналіз результативності моделі логістичної регресії на різних підгрупах даних. Наприклад, розділіть коментарі на короткі та довгі (менше або більше, ніж 50 символів), або коментарі, опубліковані в різний час доби (ранок, день, вечір). Оцініть, як модель справляється з прогнозуванням сарказму в цих підгрупах, та порівняйте її точність для кожної з них.\n",
    "\n",
    "*Технічна примітка:* Для розділення даних використайте функцію `len()` для підрахунку кількості символів у коментарях або функцію `pd.to_datetime()` для перетворення часових міток. Використайте модель `LogisticRegression()` для навчання окремих моделей для кожної групи, і порівняйте результати за допомогою `classification_report()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14:** Розробіть функцію для аналізу результативності моделі логістичної регресії з різними комбінаціями ознак (емоційне забарвлення, соціальні сигнали, частоти великих літер тощо). Створіть кілька варіантів моделей, кожна з яких використовує різні підмножини ознак, і порівняйте їх за точністю, precision, recall та F1-score. Зробіть висновки про те, які ознаки найбільше впливають на точність моделі.\n",
    "\n",
    "*Технічна примітка:* Використайте клас `LogisticRegression()` для навчання декількох моделей, кожна з яких використовує різні набори ознак. Порівняйте результати кожної моделі за допомогою `cross_val_score()` або `classification_report()` для аналізу метрик точності, precision, recall та F1-score. Для побудови моделей із різними ознаками використайте `FeatureUnion` з бібліотеки `sklearn`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15:** Застосуйте модель логістичної регресії до аналізу динаміки сарказму в певний період часу (наприклад, упродовж тижня або місяця). Проведіть аналіз, чи змінюється частота саркастичних коментарів у певні дні або в певні часові проміжки (наприклад, вечори вихідних). Використайте модель для прогнозування на основі тимчасових даних.\n",
    "\n",
    "*Технічна примітка:* Використайте метод `pd.to_datetime()` для перетворення часових міток у дні тижня або години публікації коментарів. Додайте ці часові ознаки до набору TF-IDF та навчіть модель логістичної регресії на основі цих даних. Для аналізу результативності моделі використайте `cross_val_score()` для оцінювання точності.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16:** Застосуйте модель логістичної регресії до передбачення сарказму на основі всіх ознак, що були створені раніше (емоційне забарвлення, заперечення, соціальні сигнали тощо). Після навчання моделі на основі даних *train-balanced-sarcasm.csv*, реалізуйте підхід, що використовує методи крос-валідації для оцінювання надійності моделі. Проаналізуйте результати та порівняйте їх із базовою моделлю, яка використовувала лише TF-IDF ознаки.\n",
    "\n",
    "*Технічна примітка:* Використайте `LogisticRegression()` з параметром `C` для регуляризації. Використайте `cross_val_score()` з `cv=10` для виконання крос-валідації та порівняння результатів моделі за метриками `accuracy`, `precision`, `recall` та `F1-score`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17:** Проведіть аналіз результативності моделі логістичної регресії на невідомих раніше даних. Для цього підготуйте додаткові коментарі з інших джерел, аналогічних до тих, що містяться у *train-balanced-sarcasm.csv*, та проведіть прогнозування. Проаналізуйте результати класифікації та порівняйте точність моделі на нових даних із початковою.\n",
    "\n",
    "*Технічна примітка:* Для отримання нових даних скористайтеся будь-якими доступними текстовими файлами, подібними за структурою до набору *train-balanced-sarcasm.csv*. Проведіть попередню обробку тексту (векторизацію) і застосуйте модель логістичної регресії до аналізу точності на нових даних. Порівняйте результати з оригінальними результатами моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18:** Проведіть аналіз впливу різних типів ознак на результативність моделі логістичної регресії. Створіть кілька варіантів моделі: одну з використанням тільки емоційних ознак, іншу – із соціальними сигналами, а третю – з усіма ознаками разом. Оцініть, які саме типи ознак дають найбільший приріст точності моделі для класифікації сарказму.\n",
    "\n",
    "*Технічна примітка:* Створіть окремі моделі, де використовуються лише певні типи ознак (наприклад, емоції через `TextBlob`, частота великих літер через регулярні вирази). Проведіть порівняльний аналіз результатів кожної моделі за допомогою `classification_report()` та метрик `precision`, `recall`, `F1-score`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19:** Реалізуйте стратегію побудови ансамблевої моделі, що поєднує логістичну регресію з іншими моделями (наприклад, деревом рішень або SVM) для підвищення загальної точності класифікації сарказму. Об’єднайте прогнози кількох моделей через голосування (majority voting) або обчислення середнього ймовірностей, і порівняйте результативність із базовою моделлю логістичної регресії.\n",
    "\n",
    "*Технічна примітка:* Використайте `VotingClassifier()` з параметром `voting='soft'` для комбінування результатів кількох моделей (наприклад, `LogisticRegression()`, `DecisionTreeClassifier()`, `SVC()`). Порівняйте результативність ансамблевої моделі за допомогою метрик precision, recall та F1-score.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20:** Реалізуйте стратегію для автоматичного виявлення сарказму в коментарях у режимі офлайн, застосовуючи модель логістичної регресії до великого набору даних коментарів. Проаналізуйте результати прогнозування, зокрема виведіть середній відсоток саркастичних коментарів у наборі даних і порівняйте вплив різних ознак на точність прогнозів.\n",
    "\n",
    "*Технічна примітка:* Використайте модель логістичної регресії для масового оброблення коментарів, зберігаючи результати класифікації у DataFrame. Для аналізу впливу ознак на класифікацію скористайтеся методами `coef_` для логістичної регресії або використайте бібліотеку `SHAP` для глибшого аналізу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
