{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Лабораторна робота 6.</center></h1>\n",
    "<h2><center>Лінійна регресія, Lasso і RF-регресія для задачі визначення якості вина</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Виконав:** Прізвище І.П.\n",
    "\n",
    "**Варіант:** №__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зміст\n",
    "\n",
    "- [6.1. Завантаження та оброблення даних](#lab-6.1)\n",
    "- [6.2. Лінійний регресійний аналіз](#lab-6.2)  \n",
    "- [6.3. Регресійний аналіз із регуляризацією](#lab-6.3)\n",
    "- [6.4. Побудова ансамблевих моделей](#lab-6.4)\n",
    "- [6.5. Оцінювання та відбір ознак](#lab-6.5)\n",
    "- [6.6. Покращення та подальший тюнинг моделей](#lab-6.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:55:10.883656600Z",
     "start_time": "2023-12-12T15:55:08.875194800Z"
    }
   },
   "outputs": [],
   "source": [
    "# відключимо попередження Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-6.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">6.1. Завантаження та оброблення даних</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В цій лабораторній роботі будемо працювати з набором даних за якістю білого вина ([репозиторій UCI](https://archive.ics.uci.edu/ml/datasets/wine+quality))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набір даних \"Wine Quality UCI\" включає дані, що стосуються червоних і білих сортів португальського вина \"Vinho Verde\". Цей набір даних фокусується виключно на фізико-хімічних та сенсорних змінних, не включаючи інформацію про типи винограду, марки вина та ціни продажу. Wine Quality UCI може використовуватися для задач як класифікації, так і регресії.\n",
    "\n",
    "Тут класи - впорядковані, але незбалансовані, що вказує на переважання нормальних вин над відмінними або поганими. Відповідність усіх вхідних змінних не є однозначною, що робить застосування методів відбору ознак до цієї задачі досить актуальним."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набір даних складається з 11 неперервних вхідних змінних і однієї вихідної змінної:\n",
    "\n",
    "1. Fixed Acidity (Фіксована кислотність) - відображає кількість нелетких кислот у вині.\n",
    "2. Volatile Acidity (Летюча кислотність) - кількість оцтової кислоти у вині, яка при занадто високому рівні може призвести до неприємного, схожого на оцет, смаку.\n",
    "3. Citric Acid (лимонна кислота) - у невеликих кількостях лимонна кислота може додавати вину \"свіжості\" та аромату.\n",
    "4. Residual Sugar (залишковий цукор) - кількість цукру, що залишається після припинення ферментації, рідко можна зустріти вина з вмістом цукру менше 1 грама/літр, а вина з вмістом цукру понад 45 грамів/літр вважаються солодкими.\n",
    "5. Chlorides (хлориди) - кількість солі у вині.\n",
    "6. Free Sulfur Dioxide (вільний діоксид сірки) - вільна форма SO2 існує в рівновазі між молекулярним SO2 (у вигляді розчиненого газу) і бісульфіт-іоном; він запобігає розмноженню мікроорганізмів і окисленню вина.\n",
    "7. Total Sulfur Dioxide (загальний діоксид сірки) - сума вільної та зв'язаної форм S02; у низьких концентраціях SO2 майже не відчувається у вині, але при концентрації вільного SO2 понад 50 проміле, SO2 стає помітним у носі та смаку вина.\n",
    "8. Density (щільність) - густина вина близька до густини води і залежить від вмісту алкоголю і цукру.\n",
    "9. pH - описує кислотність або лужність вина за шкалою від 0 (дуже кисле) до 14 (дуже лужне); більшість вин знаходиться між 3-4 балами за шкалою pH.\n",
    "10. Sulphates (сульфати) - добавка до вина, яка може сприяти підвищенню рівня діоксиду сірки (S02), що діє як антимікробний засіб і антиоксидант.\n",
    "11. Alcohol (алкоголь) - відсотковий вміст алкоголю у вині.\n",
    "12. Quality (якість) - вихідна змінна (цільова змінна), що базується на сенсорних даних і оцінюється від 0 до 10 балів.\n",
    "\n",
    "\n",
    "Детальний опис цього набору даних наведено у науковій роботі [Using Data Mining for Wine Quality Assessment](https://www.researchgate.net/publication/221612614_Using_Data_Mining_for_Wine_Quality_Assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:55:11.177262Z",
     "start_time": "2023-12-12T15:55:10.882656600Z"
    }
   },
   "outputs": [],
   "source": [
    "winequality_white_url = 'https://raw.githubusercontent.com/radiukpavlo/intelligent-data-analysis/refs/heads/main/02_assignments/ida_lab-06_regression-wine/winequality-white.csv'\n",
    "data = pd.read_csv(winequality_white_url, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:55:11.219978600Z",
     "start_time": "2023-12-12T15:55:11.178261200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:55:11.268377Z",
     "start_time": "2023-12-12T15:55:11.211192600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         4898 non-null   float64\n",
      " 1   volatile acidity      4898 non-null   float64\n",
      " 2   citric acid           4898 non-null   float64\n",
      " 3   residual sugar        4898 non-null   float64\n",
      " 4   chlorides             4898 non-null   float64\n",
      " 5   free sulfur dioxide   4898 non-null   float64\n",
      " 6   total sulfur dioxide  4898 non-null   float64\n",
      " 7   density               4898 non-null   float64\n",
      " 8   pH                    4898 non-null   float64\n",
      " 9   sulphates             4898 non-null   float64\n",
      " 10  alcohol               4898 non-null   float64\n",
      " 11  quality               4898 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 459.3 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виконаємо відокремлення цільової ознаки. Для цього розділимо навчальну вибірку за відношенням 7:3; тут 30% даних - тестова вибірка. Задамо `random_state=17` і виконаємо стандартизацію даних за допомогою `StandardScaler()`. Далі будемо працювати зі змінними `X_train_scaled` та `X_test_scaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:55:11.369484800Z",
     "start_time": "2023-12-12T15:55:11.245517500Z"
    }
   },
   "outputs": [],
   "source": [
    "y = data['quality']\n",
    "X = data.drop('quality', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-6.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">6.2. Лінійний регресійний аналіз</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red; font-size:1.5em;\">Завдання 1</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Для всіх варіантів:**\n",
    "\n",
    "**Побудуйте модель простої лінійної регресії за вхідними даними оцінок якості білого вина. Майте на увазі, що для навчання та тестування моделі треба використати стандартизовані значення незалежних змінних `X_train_scaled` та `X_test_scaled`, і початкові значення цільової ознаки `y_train` та `y_test`.**\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1:** Додайте до побудованої лінійної моделі поліноміальні ознаки з допомогою `PolynomialFeatures` (степені 2 і 3). Проаналізуйте, як додавання цих ознак впливає на продуктивність і стабільність моделі. Використайте регуляризацію через `Ridge`, щоби зменшити перенавчання.\n",
    "\n",
    "*Технічна примітка:* Застосуйте `PolynomialFeatures(degree=2)` та `PolynomialFeatures(degree=3)` з бібліотеки `sklearn.preprocessing`. Для регуляризації `Ridge` з `alpha=1.0` з `sklearn.linear_model`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2:** Використовуючи просту лінійну регресію, дослідіть вплив різних методів масштабування на продуктивність моделі. Порівняйте результати, отримані після оброблення даних за допомогою `StandardScaler` та `MinMaxScaler`. Оцініть модель за допомогою RMSE та R², щоби зрозуміти, як масштабування впливає на якість передбачень та на стабільність моделі.\n",
    "\n",
    "*Технічна примітка:* Використовуйте `StandardScaler()` та `MinMaxScaler()` з `sklearn.preprocessing`, метрики `mean_squared_error` та `r2_score` з `sklearn.metrics`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3:** Реалізуйте покрокову регресію з комбінацією прямого та зворотного відбору ознак для моделі лінійної регресії. Використовуйте `RFE` для автоматизованого виключення найменш значущих ознак і додайте ознаки вручну на кожному кроці. Також на кожному кроці оцінюйте точність та стабільність моделі.\n",
    "\n",
    "*Технічна примітка:* Застосуйте `RFE(estimator=LinearRegression())` з `sklearn.feature_selection`. Використовуйте `n_features_to_select=5` для обмеження кількості відібраних ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4:** Створіть ще одну модель лінійної регресії з регуляризацією ElasticNet, досліджуючи компроміс між Lasso та Ridge штрафами. Підберіть параметр `l1_ratio`, змінюючи його від 0.1 до 0.9, щоби побачити, як різні пропорції штрафів впливають на точність моделі та на значення коефіцієнтів ознак.\n",
    "\n",
    "*Технічна примітка:* Застосуйте `ElasticNet` з параметрами `l1_ratio=[0.1, 0.5, 0.9]` з `sklearn.linear_model`. Використовуйте функцію `GridSearchCV` для підбору оптимального `alpha`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5:** Проаналізуйте вплив логарифмічного перетворення викривлених ознак на продуктивність лінійної регресії. Використовуйте `numpy.log1p` для перетворення і побудуйте модель, порівнявши її з початковою для оцінювання змін у точності та стабільності.\n",
    "\n",
    "*Технічна примітка:* Використовуйте `numpy.log1p(data['feature'])` для перетворення ознак із викривленим розподілом, після чого побудуйте модель лінійної регресії.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6:** Використайте `PolynomialFeatures` з параметром `interaction_only=True`, щоб створити нові ознаки на основі взаємодії між наявними ознаками. Проаналізуйте, як додавання взаємодій впливає на точність і стабільність моделі, та виявіть найбільш значущі з них.\n",
    "\n",
    "*Технічна примітка:* Застосуйте `PolynomialFeatures(interaction_only=True)` з `sklearn.preprocessing`, після чого виконайте лінійну регресію.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7:** Застосуйте `QuantileTransformer` для трансформації ознак у вигляді нормального розподілу й побудуйте лінійну регресію. Оцініть, як така трансформація впливає на продуктивність моделі, порівнюючи її зі звичайною моделлю.\n",
    "\n",
    "*Технічна примітка:* Використовуйте `QuantileTransformer(output_distribution='normal')` з `sklearn.preprocessing`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8:** Використовуйте `RANSACRegressor` для побудови моделі надійної регресії, щоби зменшити вплив пропусків на кінцевий результат. Порівняйте продуктивність `RANSACRegressor` з результатами простої лінійної регресії та оцініть відмінності.\n",
    "\n",
    "*Технічна примітка:* Застосуйте `RANSACRegressor(base_estimator=LinearRegression())` з `sklearn.linear_model`. Використовуйте параметр `min_samples=50` для стабільності моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9:** Використайте `LassoCV` для автоматичного відбору ознак, що поєднує регресію Lasso з крос-валідацією. Це дасть змогу виявити найбільш значущі ознаки та запобігти перенавчанню. Оцініть якість моделі після такого відбору.\n",
    "\n",
    "*Технічна примітка:* Використовуйте `LassoCV(cv=5)` з `sklearn.linear_model` та відсортуйте значущі ознаки за коефіцієнтами.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10:** Після побудови моделі лінійної регресії перегляньте її коефіцієнти та відсортуйте ознаки за впливом на цільову ознаку якості вина. Розгляньте великі за модулем значення коефіцієнтів, включно з негативними, як індикатори сильного впливу.\n",
    "\n",
    "*Технічна примітка:* Використовуйте `LinearRegression().coef_` для перегляду коефіцієнтів моделі та `numpy.argsort` для сортування.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11:** Використайте метод відбору ознак на основі інформаційного критерію `mutual_info_regression` для вибору найбільш інформативних ознак. Побудуйте модель лінійної регресії з відібраними ознаками та порівняйте результати із моделлю, де використовуються всі доступні ознаки.\n",
    "\n",
    "*Технічна примітка:* Застосуйте `mutual_info_regression` з `sklearn.feature_selection` разом із `SelectKBest(score_func=mutual_info_regression, k=5)`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12:** Порівняйте продуктивність моделі лінійної регресії за використання `RobustScaler` замість стандартного масштабування. Оцініть, як впливає така зміна на стійкість моделі щодо відхилень.\n",
    "\n",
    "*Технічна примітка:* Використовуйте `RobustScaler()` з `sklearn.preprocessing` перед побудовою моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13:** Дослідіть вплив різних значень `alpha` у моделі Ridge на точність та стабільність регресії. Виберіть оптимальний `alpha` через `GridSearchCV`.\n",
    "\n",
    "*Технічна примітка:* Використовуйте `Ridge()` з `alpha` у діапазоні `[0.01, 0.05, 0.1, 1, 10]` і `GridSearchCV(cv=5)`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14:** Створіть ансамблеву модель регресії, використовуючи `BaggingRegressor` із базовою лінійною регресією. Проаналізуйте, як така модель покращує точність та зменшує вплив пропусків.\n",
    "\n",
    "*Технічна примітка:* Використовуйте `BaggingRegressor(base_estimator=LinearRegression(), n_estimators=50)` з `sklearn.ensemble`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15:** Використайте `HuberRegressor` для оброблення даних із можливими викидами. Цей метод поєднує стандартну лінійну регресію з підходом, стійким до викидів. Порівняйте продуктивність `HuberRegressor` зі звичайною лінійною регресією.\n",
    "\n",
    "*Технічна примітка:* Використовуйте `HuberRegressor(epsilon=1.35)` з `sklearn.linear_model`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16:** Проведіть аналіз впливу різних порогових значень у функції відбору ознак SelectFromModel, застосовуючи її до моделі лінійної регресії з регуляризацією Lasso. Порівняйте результати продуктивності залежно від вибраних ознак.\n",
    "\n",
    "*Технічна примітка:* Використовуйте `SelectFromModel(estimator=Lasso(alpha=0.01))` з `sklearn.feature_selection`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17:** Використайте `SelectKBest` для відбору найкращих ознак на основі статистичної міри. Застосуйте тест `f_regression`, щоби відібрати ознаки з найсильнішим впливом на цільову змінну, і побудуйте модель лінійної регресії, використовуючи тільки відібрані ознаки. Порівняйте продуктивність моделі до й після відбору ознак.\n",
    "\n",
    "*Технічна примітка:* Використовуйте `SelectKBest(score_func=f_regression, k=5)` з `sklearn.feature_selection`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18:** Проведіть відбір ознак на основі важливості за допомогою `RandomForestRegressor`, а потім використайте ці важливі ознаки для побудови лінійної регресійної моделі. Дослідіть, як вибір лише найбільш значущих ознак із моделі випадкового лісу впливає на точність та стабільність лінійної регресії.\n",
    "\n",
    "*Технічна примітка:* Використовуйте `RandomForestRegressor()` з `sklearn.ensemble` для визначення важливості ознак та `SelectFromModel` для їх відбору.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19:** Використайте `Recursive Feature Elimination with Cross-Validation (RFECV)` для автоматичного визначення оптимальної кількості ознак. Побудуйте лінійну регресійну модель із відібраними ознаками та оцініть її продуктивність, порівнюючи її з повною моделлю, щоби побачити, чи зміниться точність при відборі ознак.\n",
    "\n",
    "*Технічна примітка:* Застосуйте `RFECV(estimator=LinearRegression(), cv=5)` з `sklearn.feature_selection`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20:** Використайте `L1-базований відбір ознак`, застосувавши Lasso регресію для видалення ознак із незначним впливом. Порівняйте продуктивність моделі до та після видалення ознак. Цей метод допоможе зменшити розмірність моделі та покращити її інтерпретованість.\n",
    "\n",
    "*Технічна примітка:* Використовуйте `Lasso(alpha=0.01, max_iter=10000)` з `sklearn.linear_model` і `SelectFromModel` для відбору значущих ознак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-6.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">6.3. Регресійний аналіз із регуляризацією</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red; font-size:1.5em;\">Завдання 2</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Для всіх варіантів:**\n",
    "\n",
    "**Побудуйте моделі Lasso-регресі з параметрами `alpha=0.01`, `random_state=17` та `cv=5`. Обрахуйте вплив усіх незалежних ознак на цільову ознаку Lasso-регресії. Відсортуйте ознаки за впливом на якість вина. Майте на увазі, що для навчання та тестування моделі треба використати стандартизовані значення незалежних змінних `X_train_scaled` та `X_test_scaled`, і початкові значення цільової ознаки `y_train` та `y_test`.**\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1:** Виконайте відбір ознак на основі їхньої важливості для Lasso-регресії. Після побудови моделі за допомогою Lasso проаналізуйте коефіцієнти отриманих ознак: залиште тільки ті, що мають ненульові значення. Потім порівняйте продуктивність отриманої моделі з моделлю, що використовує всі ознаки, та оцініть, наскільки вибір основних ознак впливає на точність передбачень.\n",
    "\n",
    "*Технічна примітка:* Використайте `Lasso(alpha=0.01)` для отримання важливих ознак та `SelectFromModel` з `sklearn.feature_selection`, щоб залишити тільки ті, які мають значний вплив.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2:** Після побудови моделі Lasso-регресії оберіть ознаки за допомогою методу `SelectKBest`, що базується на статистичній значущості кожної ознаки. Проаналізуйте, як вибір найвпливовіших ознак на основі їхніх значень у моделі впливає на точність прогнозування. Це допоможе визначити оптимальну кількість ознак для передбачення цільової змінної.\n",
    "\n",
    "*Технічна примітка:* Використайте `SelectKBest(score_func=f_regression, k=5)` з `sklearn.feature_selection` для відбору ознак на основі значень Lasso.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3:** Застосуйте метод зворотного виключення ознак (RFE) для побудови Lasso-регресії. Методично видаляйте ознаки з найменшим впливом, поки не залишаться тільки найбільш важливі. Порівняйте результати продуктивності моделі зі зворотним виключенням і без нього, щоби зрозуміти, як відбір ознак впливає на передбачення.\n",
    "\n",
    "*Технічна примітка:* Використайте `RFE(estimator=Lasso(alpha=0.01))` з `sklearn.feature_selection` для покрокового видалення ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4:** Проведіть груповий аналіз ознак для відбору найбільш впливових груп за допомогою GroupLasso. Створіть модель, у якій деякі ознаки об’єднані в групи. Порівняйте, як модель обробляє ці групи порівняно з окремими ознаками, та оцініть, чи сприяє групування покращенню передбачуваності.\n",
    "\n",
    "*Технічна примітка:* Для реалізації Використайте `group-lasso` з груповими ознаками або `GroupLassoCV`, щоби спростити відбір груп ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5:** Дослідіть вплив кожної ознаки на цільову змінну за допомогою побудови Lasso-регресії з різними значеннями параметра `alpha`. Оцініть, як змінюється кількість ознак із ненульовими коефіцієнтами у випадку збільшення `alpha`, та визначте оптимальний рівень регуляризації для відбору найбільш впливових ознак.\n",
    "\n",
    "*Технічна примітка:* Застосуйте `LassoCV` з параметром `alphas=np.linspace(0.01, 1.0, 10)` для пошуку оптимального значення регуляризації.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6:** Виконайте відбір ознак на основі важливості за допомогою випадкового лісу, а потім використайте відібрані ознаки для побудови моделі Lasso-регресії. Оцініть, як поєднання випадкового лісу для відбору та Lasso-регресії для моделювання впливає на точність передбачень і стабільність моделі.\n",
    "\n",
    "*Технічна примітка:* Використайте `RandomForestRegressor()` для обчислення важливості ознак та `SelectFromModel` для їхнього відбору перед застосуванням `Lasso(alpha=0.01)`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7:** Дослідіть вплив відбору ознак на основі кореляційного аналізу, залишивши лише ті ознаки, які мають високий кореляційний зв’язок із цільовою змінною. Потім побудуйте модель Lasso-регресії з відфільтрованими ознаками та порівняйте її результати з моделлю, що використовує всі ознаки.\n",
    "\n",
    "*Технічна примітка:* Використайте `data.corr()` з бібліотеки `pandas` для обчислення кореляції, щоби залишити тільки значущі ознаки перед застосуванням `Lasso(alpha=0.01)`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8:** Використайте метод відбору ознак `mutual_info_regression` для аналізу взаємної інформації кожної ознаки із цільовою змінною. Виберіть тільки найбільш інформативні ознаки, а потім побудуйте модель Lasso-регресії, щоб оцінити, як використання обраних ознак впливає на передбачення.\n",
    "\n",
    "*Технічна примітка:* Використайте `mutual_info_regression` з `sklearn.feature_selection` та `SelectKBest(score_func=mutual_info_regression, k=5)`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9:** Після побудови моделі Lasso-регресії перегляньте її коефіцієнти та відсортуйте ознаки за їхнім впливом на цільову змінну. Використайте великі за модулем значення коефіцієнтів для визначення найважливіших ознак. Створіть візуалізацію впливу ознак за допомогою графіка стовпців.\n",
    "\n",
    "*Технічна примітка:* Використайте `LinearRegression().coef_` для перегляду коефіцієнтів і `matplotlib.pyplot.bar` для візуалізації впливу ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10:** Дослідіть, як стабільність ознак змінюється в процесі регуляризації Lasso-регресії, створюючи кілька бутстреп-вибірок початкового набору даних. Перегляньте, які ознаки постійно залишаються значущими у всіх вибірках, щоби виділити найстабільніші ознаки для моделювання.\n",
    "\n",
    "*Технічна примітка:* Використайте функцію `resample` з `sklearn.utils` для створення бутстреп-вибірок та `Lasso(alpha=0.01)` для моделювання впливу ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11:** Використайте метод головних компонент (PCA) для відбору найбільш значущих ознак та побудуйте модель Lasso-регресії на зменшеному наборі ознак. Проаналізуйте, як зменшення кількості ознак за допомогою PCA впливає на точність передбачення та на інтерпретованість моделі.\n",
    "\n",
    "*Технічна примітка:* Використайте `PCA(n_components=5)` з `sklearn.decomposition` для відбору основних компонент перед застосуванням `Lasso(alpha=0.01)`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12:** Використайте Lasso-регресію для аналізу взаємодій між ознаками, поєднуючи її з методом взаємної інформації. Спочатку визначте найбільш інформативні ознаки за допомогою `mutual_info_regression`, а потім застосуйте Lasso, щоби проаналізувати вплив взаємодій між обраними ознаками на точність передбачення.\n",
    "\n",
    "*Технічна примітка:* Використайте `mutual_info_regression` з `SelectKBest` для попереднього відбору ознак, а потім `PolynomialFeatures(interaction_only=True)` з `Lasso(alpha=0.01)`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13:** Виконайте відбір ознак методом послідовного додавання (forward selection) для побудови моделі Lasso-регресії. Додавайте ознаки почергово, оцінюючи їхній вплив на точність моделі. Цей підхід дасть змогу визначити оптимальний набір ознак, які найбільше впливають на передбачення цільової змінної.\n",
    "\n",
    "*Технічна примітка:* Використайте `SequentialFeatureSelector` з параметром `direction='forward'` з `sklearn.feature_selection`, а потім застосуйте `Lasso(alpha=0.01)` для моделювання.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14:** Проведіть експеримент з Lasso-регресією, використовуючи різні вагові коефіцієнти для кожної ознаки. Задайте вищі ваги для ознак, які є найбільш значущими за попереднім аналізом, й оцініть, як це впливає на продуктивність моделі та вибір основних ознак.\n",
    "\n",
    "*Технічна примітка:* Використайте параметр `sample_weight` в `Lasso(alpha=0.01)` з `sklearn.linear_model` для коригування вагових коефіцієнтів ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15:** Побудуйте модель Lasso-регресії з урахуванням зворотного виключення ознак на основі розрахунку їхньої інформаційної значущості. Видаляйте ознаки, які найменш інформативні на кожному етапі, та залиште лише найбільш значущі для фінальної моделі. Оцініть стабільність та точність отриманих результатів.\n",
    "\n",
    "*Технічна примітка:* Використайте `RFE(estimator=Lasso(alpha=0.01))` для поетапного видалення ознак та `mutual_info_regression` для визначення їх значущості.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16:** Застосуйте багаторазовий метод відбору ознак на основі випадкових вибірок. Згенеруйте кілька випадкових вибірок ознак і побудуйте модель Lasso-регресії для кожної з них. Оцініть, які ознаки найбільш стійко обираються в різних вибірках, і залиште їх для остаточної моделі.\n",
    "\n",
    "*Технічна примітка:* Використайте `RandomizedSearchCV` з `Lasso(alpha=0.01)` для створення випадкових вибірок ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17:** Використайте метод `SelectFdr` (False Discovery Rate) для відбору ознак, які найбільш значущо корелюють із цільовою змінною. Після обрання ознак побудуйте модель Lasso-регресії та проаналізуйте, як відбір за значущістю впливає на точність передбачень.\n",
    "\n",
    "*Технічна примітка:* Використайте `SelectFdr(score_func=f_regression, alpha=0.05)` з `sklearn.feature_selection` для відбору ознак, а потім застосуйте `Lasso(alpha=0.01)`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18:** Побудуйте модель Lasso-регресії з крос-валідацією для оцінювання точності та стабільності відбору ознак. Використайте `cross_val_score` для обрахунку середніх показників точності на кожному етапі, щоби виявити, які ознаки залишаються найбільш значущими незалежно від розподілу даних.\n",
    "\n",
    "*Технічна примітка:* Використайте `cross_val_score(Lasso(alpha=0.01), X, y, cv=5)` з `sklearn.model_selection` для проведення крос-валідації.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19:** Виконайте відбір ознак за допомогою методу «моделювання ансамблів», де декілька моделей (Lasso, Ridge та Decision Tree) обирають найбільш значущі ознаки. Проаналізуйте, які ознаки стабільно залишаються значущими в різних моделях, та використайте їх у моделі Lasso-регресії для точного передбачення.\n",
    "\n",
    "*Технічна примітка:* Використайте `VotingRegressor([('lasso', Lasso(alpha=0.01)), ('ridge', Ridge()), ('tree', DecisionTreeRegressor())])` з `sklearn.ensemble`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20:** Застосуйте адаптивну Lasso-регресію з ваговими коефіцієнтами для кожної ознаки на основі їхньої початкової важливості. Визначте початкові коефіцієнти, використовуючи іншу регресійну модель, а потім скоригуйте ваги для Lasso, щоби зробити модель більш гнучкою до впливових ознак.\n",
    "\n",
    "*Технічна примітка:* Використайте `Lasso(alpha=0.01)` для початкового відбору ознак і `sample_weight` для вагових коефіцієнтів під час побудови остаточної адаптивної Lasso-регресії."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-6.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">6.4. Побудова ансамблевих моделей</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red; font-size:1.5em;\">Завдання 3</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Для всіх варіантів:**\n",
    "\n",
    "**Побудуйте модель випадкового лісу з гіперпараметрами за замовчуванням. Для навчання та тестування моделі треба використати стандартизовані значення незалежних змінних `X_train_scaled` та `X_test_scaled`, і початкові значення цільової ознаки `y_train` та `y_test`.**\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1:** Використайте `GridSearchCV` для ретельного налаштування гіперпараметрів моделі випадкового лісу. Дослідіть такі параметри, як `n_estimators` (кількість дерев), `max_depth` (максимальна глибина дерева) та `min_samples_split` (мінімальна кількість зразків для розбиття вузла). Оцініть, як кожен параметр впливає на точність моделі для визначення якості вина.\n",
    "\n",
    "*Технічна примітка:* Використайте `GridSearchCV` з параметром `cv=5` для налаштування крос-валідації та параметрами діапазону для `n_estimators=[50, 100, 150]`, `max_depth=[10, 20, None]`, `min_samples_split=[2, 5, 10]` у `sklearn.ensemble.RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2:** Побудуйте та порівняйте дві моделі — Random Forest та Extra Trees. Проаналізуйте, як кожен з ансамблевих методів (беггінг у випадковому лісі та випадковий розщеп в Extra Trees) впливає на загальну результативність і стабільність моделей. Зробіть висновки щодо того, яка модель краще підходить для даного набору даних.\n",
    "\n",
    "*Технічна примітка:* Використайте `RandomForestRegressor` та `ExtraTreesRegressor` з `sklearn.ensemble`, порівнюючи метрики `mean_squared_error` та `r2_score`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3:** Застосуйте моделі беггінгу для логістичної регресії та дерев рішень, а також випадковий ліс, щоби порівняти їхні результати на основі метрики AUC ROC. Визначте, яка з моделей має найкращу здатність відокремлювати класи для даних про якість вина.\n",
    "\n",
    "*Технічна примітка:* Використайте `BaggingClassifier` з базовими моделями `LogisticRegression` і `DecisionTreeClassifier`, а також `RandomForestClassifier` з `roc_auc_score` з `sklearn.metrics`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4:** Для оброблення незбалансованих класів у наборі даних Використайте `BalancedRandomForestClassifier` з бібліотеки `imbalanced-learn`. Оцініть вплив автоматичного балансування на результативність моделі та порівняйте його з результатами звичайного випадкового лісу.\n",
    "\n",
    "*Технічна примітка:* Використайте `BalancedRandomForestClassifier` з `imblearn.ensemble` та `RandomForestClassifier` з `class_weight='balanced'` для порівняння результативності.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5:** Проведіть дослідження на основі різних значень глибини дерев у моделі випадкового лісу. Виконайте експерименти зі значеннями `max_depth` від 5 до 30 і оцініть, як різні рівні глибини впливають на точність, обчислювальну складність та узагальненість моделі.\n",
    "\n",
    "*Технічна примітка:* Застосуйте параметр `max_depth` зі значеннями `[5, 10, 20, 30, None]` у `RandomForestClassifier` або `RandomForestRegressor`, використовуючи метрики `accuracy_score` або `mean_squared_error`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6:** Дослідіть, як вибір критерію розбиття вузлів у випадковому лісі (наприклад, `gini` та `entropy`) впливає на результативність моделі. Застосуйте обидва критерії, побудуйте дві моделі та порівняйте їх за метриками точності, щоби визначити, який критерій підходить краще.\n",
    "\n",
    "*Технічна примітка:* Використайте параметр `criterion='gini'` та `criterion='entropy'` з `RandomForestClassifier`, порівнюючи точність за допомогою `accuracy_score` з `sklearn.metrics`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7:** Перед побудовою моделі випадкового лісу виконайте відбір ознак за допомогою `SelectFromModel` із застосуванням Lasso або `Recursive Feature Elimination (RFE)`. Оцініть, як обрані ознаки впливають на результативність моделі, і порівняйте точність із моделлю, побудованою на повному наборі ознак.\n",
    "\n",
    "*Технічна примітка:* Використайте `SelectFromModel` з `Lasso(alpha=0.01)` або `RFE(estimator=RandomForestClassifier())` з `sklearn.feature_selection`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8:** Експериментуйте з різними стратегіями вибірки, такими як `bootstrap=True` та `bootstrap=False`, під час створення дерев у випадковому лісі. Проаналізуйте, як ці стратегії впливають на зміщення та дисперсію моделі, а також на її здатність до узагальнення.\n",
    "\n",
    "*Технічна примітка:* Використайте параметр `bootstrap=True` та `bootstrap=False` у `RandomForestClassifier` та порівняйте метрики `accuracy_score` і `f1_score`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9:** Побудуйте моделі беггінгу для логістичної регресії та дерев рішень, порівнюючи їх із випадковим лісом за допомогою показників середньоквадратичного відхилення точності. Це дасть змогу оцінити, яка з моделей має стабільнішу результативність.\n",
    "\n",
    "*Технічна примітка:* Використайте `BaggingClassifier` з `LogisticRegression` та `DecisionTreeClassifier`, а також `RandomForestClassifier`. Розрахуйте середньоквадратичне відхилення точності через `cross_val_score`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10:** Використайте модель випадкового лісу з балансуванням ваги класу, налаштувавши параметр `class_weight='balanced'`. Порівняйте результативність моделі з і без bootstrap-вибірок, щоби побачити вплив балансування та різних стратегій вибірки на точність передбачень.\n",
    "\n",
    "*Технічна примітка:* Використайте параметр `class_weight='balanced'` у `RandomForestClassifier` з `bootstrap=True` та `bootstrap=False`, і порівняйте результати за допомогою `roc_auc_score` та `accuracy_score`.\n",
    "\n",
    "**Варіант 11:** Виконайте налаштування гіперпараметрів моделі випадкового лісу з використанням `RandomizedSearchCV`. Дослідіть широкий діапазон значень параметрів `n_estimators`, `max_features`, `min_samples_leaf` та `max_depth`, щоб знайти оптимальні налаштування для найкращої результативності моделі на завданні класифікації якості вина.\n",
    "\n",
    "*Технічна примітка:* Використайте `RandomizedSearchCV` з параметрами `n_iter=50` та `cv=5` для вибору випадкових гіперпараметрів. Для налаштування Використайте `n_estimators=[50, 100, 150, 200]`, `max_features=['auto', 'sqrt']`, `min_samples_leaf=[1, 2, 4]`, та `max_depth=[10, 20, 30, None]`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12:** Порівняйте випадковий ліс із градієнтним бустингом (`GradientBoostingClassifier`) для завдання класифікації якості вина. Проведіть аналіз на основі метрик точності та обчислювальної ефективності, щоб зрозуміти переваги та недоліки кожного методу для даного набору даних.\n",
    "\n",
    "*Технічна примітка:* Використайте `RandomForestClassifier` і `GradientBoostingClassifier` з `sklearn.ensemble`, оцінюючи моделі за допомогою `accuracy_score` та `f1_score`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13:** Використайте метод `SelectKBest` для відбору ознак перед побудовою моделі випадкового лісу. Досліджуйте, як зменшення кількості ознак на основі статистичної значущості впливає на результативність моделі та її узагальнення.\n",
    "\n",
    "*Технічна примітка:* Використайте `SelectKBest(score_func=f_classif, k=10)` з `sklearn.feature_selection` для відбору ознак і `RandomForestClassifier` для побудови моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14:** Виконайте аналіз стабільності моделі випадкового лісу, застосовуючи крос-валідацію з різними значеннями `random_state`. Оцініть, як зміни в початковому випадковому стані впливають на стабільність точності та структуру дерев у моделі.\n",
    "\n",
    "*Технічна примітка:* Використайте `cross_val_score` з різними значеннями `random_state` у `RandomForestClassifier`, наприклад `[0, 42, 99, 123]`, та оцінюйте середнє значення точності.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15:** Проведіть експеримент із частковим ансамблюванням у випадковому лісі, використовуючи лише частину ознак під час побудови кожного дерева (`max_features`). Оцініть, як варіація значень `max_features` впливає на результативність моделі та її обчислювальну ефективність.\n",
    "\n",
    "*Технічна примітка:* Використайте `RandomForestClassifier` із параметром `max_features=[0.3, 0.5, 'auto', 'sqrt']` для оцінювання впливу часткового ансамблювання.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16:** Дослідіть вплив регуляризації моделі випадкового лісу, налаштувавши параметри `min_samples_split` та `min_samples_leaf`. Проведіть аналіз, як збільшення значень цих параметрів впливає на узагальненість моделі, її точність та складність структури дерев.\n",
    "\n",
    "*Технічна примітка:* Використайте `RandomForestClassifier` із параметрами `min_samples_split=[2, 5, 10]` та `min_samples_leaf=[1, 2, 4]`, оцінюючи точність за допомогою `accuracy_score`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17:** Використайте `Feature Importance` з випадкового лісу для визначення найбільш впливових ознак у наборі даних. Після визначення ключових ознак побудуйте модель тільки на основі цих ознак і порівняйте результативність із повною моделлю.\n",
    "\n",
    "*Технічна примітка:* Використайте атрибут `feature_importances_` у `RandomForestClassifier` для визначення важливості ознак та `SelectFromModel` для вибору найбільш значущих ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18:** Виконайте кластеризацію ознак за допомогою `FeatureAgglomeration` перед побудовою випадкового лісу. Створіть менший набір ознак на основі кластерів і проаналізуйте, як це впливає на результативність і швидкість моделі.\n",
    "\n",
    "*Технічна примітка:* Використайте `FeatureAgglomeration` з `sklearn.cluster` для зменшення кількості ознак перед застосуванням `RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19:** Використайте `Stochastic Gradient Boosting (SGD)` разом із випадковим лісом, щоби побудувати модель гібридного ансамблю. Проаналізуйте, як цей підхід впливає на точність та узагальнення проти класичних випадкових лісів.\n",
    "\n",
    "*Технічна примітка:* Використайте `VotingClassifier` з моделями `RandomForestClassifier` та `SGDClassifier` з параметром `voting='soft'` з `sklearn.ensemble`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20:** Застосуйте стратегію розширення ознак, додаючи до початкових ознак статистичні показники, як-от середнє, стандартне відхилення та медіану. Побудуйте модель випадкового лісу з розширеним набором ознак і оцініть, чи допомагає додаткова інформація поліпшити точність передбачень.\n",
    "\n",
    "*Технічна примітка:* Використайте функції `mean()`, `std()` та `median()` з `pandas` для створення нових ознак, потім застосуйте `RandomForestClassifier` для моделювання."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-6.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">6.5. Оцінювання та відбір ознак</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red; font-size:1.5em;\">Завдання 4</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1:** Використайте `Principal Component Analysis (PCA)` для зменшення розмірності набору ознак перед побудовою моделі випадкового лісу. Перетворіть початкові ознаки на основні компоненти та оберіть перші компоненти, що пояснюють значну частину дисперсії (>90 %). Порівняйте результативність моделі, побудованої на основних компонентах, з моделлю, побудованою на початкових ознаках.\n",
    "\n",
    "*Технічна примітка:* Використайте `PCA(n_components=10)` з `sklearn.decomposition` для створення основних компонент і `RandomForestClassifier` для побудови моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2:** Виконайте `Linear Discriminant Analysis (LDA)` для зменшення розмірності ознак, зберігаючи інформацію про класи. Виберіть найбільш дискримінуючі компоненти для побудови моделі випадкового лісу та оцініть, як використання LDA впливає на точність передбачення порівняно з повним набором ознак.\n",
    "\n",
    "*Технічна примітка:* Застосуйте `LinearDiscriminantAnalysis(n_components=2)` з `sklearn.discriminant_analysis` до вибору найбільш значущих компонент і Використайте `RandomForestClassifier` для моделювання.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3:** Застосуйте метод `Recursive Feature Addition` до побудови моделі випадкового лісу з поступовим додаванням ознак. На кожному етапі додавайте ознаку, яка найбільше покращує результативність моделі, і продовжуйте, допоки не досягнете бажаного рівня точності. Оцініть остаточний набір обраних ознак.\n",
    "\n",
    "*Технічна примітка:* Використайте `SequentialFeatureSelector` з параметром `direction='forward'` у `sklearn.feature_selection`, а потім `RandomForestClassifier` для побудови остаточної моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4:** Здійсніть відбір ознак на основі взаємного інформаційного індексу, щоб відібрати ознаки з найвищою мірою залежності із цільовою змінною. Оберіть найінформативніші ознаки для побудови моделі випадкового лісу й порівняйте результат із моделлю, що використовує повний набір ознак.\n",
    "\n",
    "*Технічна примітка:* Використайте `mutual_info_classif` з `SelectKBest` у `sklearn.feature_selection` для відбору найбільш значущих ознак перед реалізуванням `RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5:** Реалізуйте побудову нових ознак через комбінацію поліноміальних взаємодій. Використайте `PolynomialFeatures` для створення взаємодій між основними ознаками, а потім побудуйте модель випадкового лісу на новому наборі ознак. Проаналізуйте, як додавання поліноміальних взаємодій впливає на результативність моделі.\n",
    "\n",
    "*Технічна примітка:* Використайте `PolynomialFeatures(degree=2, interaction_only=True)` з `sklearn.preprocessing` для створення взаємодій перед реалізуванням `RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6:** Використайте метод відбору ознак на основі об’єднання показників кореляції та важливості ознак у моделі випадкового лісу. Спершу виберіть ознаки, що мають високу кореляцію із цільовою змінною, а потім Використайте `SelectFromModel` для подальшого відбору. Побудуйте остаточну модель на основі цих обраних ознак.\n",
    "\n",
    "*Технічна примітка:* Використайте `data.corr()` з бібліотеки `pandas` для обчислення кореляції та `SelectFromModel(RandomForestClassifier())` для подальшого відбору ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7:** Реалізуйте кластеризацію ознак за допомогою `FeatureAgglomeration` для групування ознак зі схожими характеристиками. Побудуйте нову модель випадкового лісу на основі зменшеного набору ознак і проаналізуйте, як кластеризація впливає на результативність і стабільність моделі.\n",
    "\n",
    "*Технічна примітка:* Використайте `FeatureAgglomeration(n_clusters=10)` з `sklearn.cluster` для кластеризації ознак перед реалізуванням `RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8:** Створіть нові ознаки, розрахувавши середні значення, медіану та стандартне відхилення для груп споріднених ознак. Побудуйте модель випадкового лісу на розширеному наборі ознак та оцініть, як додавання статистичних характеристик впливає на результативність передбачень.\n",
    "\n",
    "*Технічна примітка:* Використайте `mean()`, `median()` та `std()` з бібліотеки `pandas` для обчислення нових ознак і `RandomForestClassifier` для моделювання.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9:** Виконайте відбір ознак на основі їхньої ваги в моделі випадкового лісу, зберігаючи тільки ті ознаки, які мають вагу понад певний поріг. Побудуйте остаточну модель на основі обраних ознак і порівняйте її точність із початковою моделлю.\n",
    "\n",
    "*Технічна примітка:* Використайте `feature_importances_` у `RandomForestClassifier` для визначення ваг ознак і `SelectFromModel` для їхнього відбору.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10:** Виконайте перетворення вхідних ознак за допомогою `QuantileTransformer`, щоби привести їх до нормального розподілу. Порівняйте точність моделі випадкового лісу, побудованої на перетворених ознаках, з моделлю, що використовує початкові ознаки, для оцінювання впливу нормалізації на результативність.\n",
    "\n",
    "*Технічна примітка:* Використайте `QuantileTransformer(output_distribution='normal')` з `sklearn.preprocessing` перед реалізуванням `RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11:** Використайте `VarianceThreshold` для відбору ознак із високою варіативністю, щоб виключити ознаки з низькою інформаційною цінністю. Це дасть змогу зменшити розмірність даних і зосередитися на найбільш значущих ознаках, перш ніж будувати модель випадкового лісу.\n",
    "\n",
    "*Технічна примітка:* Використайте `VarianceThreshold(threshold=0.1)` з `sklearn.feature_selection` для відбору ознак, які мають варіативність понад вказаний поріг, а потім побудуйте модель з `RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12:** Застосуйте метод відбору ознак `Lasso` до моделі випадкового лісу перед її побудовою, щоб виключити менш значущі ознаки. Відібрані ознаки, зокрема з ненульовими ваговими коефіцієнтами, будуть використовуватися для подальшого аналізу й порівняння результативності з повною моделлю.\n",
    "\n",
    "*Технічна примітка:* Використайте `Lasso(alpha=0.01)` з `SelectFromModel` у `sklearn.feature_selection` для відбору ознак перед реалізуванням `RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13:** Використайте `SelectFpr` для відбору ознак із високим рівнем статистичної значущості (за рівнем похибки першого роду). Оберіть найважливіші ознаки для подальшого моделювання та порівняйте точність передбачень із початковою моделлю.\n",
    "\n",
    "*Технічна примітка:* Використайте `SelectFpr(score_func=f_classif, alpha=0.05)` з `sklearn.feature_selection` для відбору значущих ознак перед побудовою моделі з `RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14:** Використайте комбінацію відбору ознак на основі важливості (важливість перестановок) та статистичних показників. Спершу використайте `SelectKBest` для визначення найінформативніших ознак, а потім підтвердіть їх важливість через `permutation_importance` для моделі випадкового лісу.\n",
    "\n",
    "*Технічна примітка:* Використайте `SelectKBest(score_func=f_classif, k=10)` та `permutation_importance` з `sklearn.inspection` перед остаточним реалізуванням `RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15:** Створіть нові ознаки, об’єднуючи початкові ознаки в пари через добуток значень. Це дасть змогу побудувати нові нелінійні взаємозв’язки, які можуть бути значущими для моделі випадкового лісу. Порівняйте результативність нової моделі з оригінальною.\n",
    "\n",
    "*Технічна примітка:* Використайте `PolynomialFeatures(degree=2, include_bias=False)` з `sklearn.preprocessing` для створення добутків значень ознак перед реалізуванням `RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16:** Використайте метод `Univariate Feature Selection`, щоби зберегти тільки ознаки з найвищим показником значущості. Застосуйте критерій `chi2` до визначення найінформативніших ознак, а потім побудуйте модель випадкового лісу на відфільтрованих ознаках.\n",
    "\n",
    "*Технічна примітка:* Використайте `SelectKBest(score_func=chi2, k=10)` з `sklearn.feature_selection` для відбору ознак перед реалізуванням `RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17:** Виконайте відбір ознак на основі `Ridge Regression`, щоби зменшити кількість ознак, зберігаючи тільки ті, які найбільше впливають на цільову змінну. Порівняйте результат моделі випадкового лісу з обраними ознаками та з повним набором ознак.\n",
    "\n",
    "*Технічна примітка:* Використайте `Ridge(alpha=1.0)` з `SelectFromModel` для відбору ознак, а потім реалізуйте `RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18:** Створіть комбінації ознак на основі кластеризації, застосувавши `KMeans` до початкових ознак. Використайте результуючі кластери як нові ознаки та побудуйте модель випадкового лісу на основі зменшеного набору, щоб оцінити, як групування ознак впливає на точність передбачень.\n",
    "\n",
    "*Технічна примітка:* Використайте `KMeans(n_clusters=5)` з `sklearn.cluster` для створення кластерів, а потім `RandomForestClassifier` для побудови моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19:** Дослідіть вплив нормалізації ознак, застосовуючи до них `StandardScaler`, `RobustScaler` та `MinMaxScaler`, перед побудовою моделі випадкового лісу. Порівняйте точність і стабільність передбачень для кожного типу нормалізації, щоби зрозуміти, яка з них найбільше підходить для вашого набору даних.\n",
    "\n",
    "*Технічна примітка:* Використайте `StandardScaler()`, `RobustScaler()`, та `MinMaxScaler()` з `sklearn.preprocessing`, застосовуючи до них `RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20:** Використайте метод відбору ознак на основі `Maximal Information Coefficient (MIC)` для визначення ознак із високим рівнем нелінійної залежності із цільовою змінною. Використайте відібрані ознаки для побудови моделі випадкового лісу та порівняйте точність із моделлю на початковому наборі ознак.\n",
    "\n",
    "*Технічна примітка:* Використайте бібліотеку `minepy` для обчислення MIC та відбору ознак перед реалізуванням `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-6.6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">6.6. Покращення та подальший тюнинг моделей</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red; font-size:1.5em;\">Завдання 5</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1:** Використайте метод `Gradient Boosting` для побудови ансамблевої моделі та виконайте відбір ознак на основі важливості, наданої цією моделлю. Збережіть ознаки з високим ступенем важливості та порівняйте результативність моделі, побудованої на відібраних ознаках, з моделлю на повному наборі ознак.\n",
    "\n",
    "*Технічна примітка:* Використайте `GradientBoostingClassifier` із параметром `n_estimators=100`, а потім `feature_importances_` для відбору ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2:** Проведіть `AdaBoost` із базовими моделями, такими як дерева рішень або логістична регресія, та виконайте трансформацію ознак на основі важливості, яку надає ця модель. Це дасть можливість зрозуміти, як зберігати тільки впливові ознаки, що значно впливають на точність передбачень у цій моделі.\n",
    "\n",
    "*Технічна примітка:* Використайте `AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1))` з `feature_importances_` для визначення важливих ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3:** Застосуйте метод багатовимірного шкалювання (`MDS`) до перетворення ознак у простір меншої розмірності перед побудовою моделі. Це дасть можливість краще зрозуміти структуру даних та оцінити, як зменшення розмірності впливає на результативність моделі ансамблю.\n",
    "\n",
    "*Технічна примітка:* Використайте `MDS(n_components=5)` з `sklearn.manifold`, щоб зменшити розмірність, а потім побудуйте модель із `RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4:** Виконайте кластеризацію ознак за допомогою `Agglomerative Clustering`, щоб згрупувати ознаки з подібними характеристиками, а потім створіть нові групові ознаки. Побудуйте модель ансамблю на основі цих нових груп і проаналізуйте, чи зберігаються значущі зв’язки та, чи покращується точність.\n",
    "\n",
    "*Технічна примітка:* Використайте `AgglomerativeClustering(n_clusters=10)` з `sklearn.cluster`, щоб створити нові групи ознак перед реалізуванням `GradientBoostingClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5:** Виконайте відбір ознак за допомогою `Recursive Feature Elimination (RFE)` на основі моделі `Gradient Boosting`. Це дасть можливість залишити лише найбільш важливі ознаки, що сприяють покращенню точності передбачень. Порівняйте результати моделі до та після відбору ознак.\n",
    "\n",
    "*Технічна примітка:* Використайте `RFE(estimator=GradientBoostingClassifier(), n_features_to_select=10)` з `sklearn.feature_selection`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6:** Реалізуйте трансформацію ознак через метод `Isomap` для збереження нелінійних зв’язків між ознаками, зменшуючи водночас їхню розмірність. Це дасть можливість зберегти основну інформацію та побудувати компактнішу модель для побудови ансамблю.\n",
    "\n",
    "*Технічна примітка:* Використайте `Isomap(n_components=5)` з `sklearn.manifold`, щоб зменшити розмірність ознак, а потім реалізуйте `AdaBoostClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7:** Застосуйте `SelectFromModel` з моделлю `Random Forest` до відбору найважливіших ознак, що сприяють точності передбачення. Використайте модель із відфільтрованими ознаками для побудови іншого ансамблю, наприклад, `Gradient Boosting`, і порівняйте результати з повною моделлю.\n",
    "\n",
    "*Технічна примітка:* Використайте `SelectFromModel(estimator=RandomForestClassifier())` з `sklearn.feature_selection`, щоб відсіяти маловпливові ознаки, та `GradientBoostingClassifier` для подальшого моделювання.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8:** Виконайте поліноміальну трансформацію ознак другого ступеня, зберігаючи тільки найвпливовіші поліноміальні взаємодії. Побудуйте модель ансамблю, наприклад, `AdaBoost`, на новому наборі ознак та порівняйте результати з моделлю, побудованою на оригінальних ознаках.\n",
    "\n",
    "*Технічна примітка:* Використайте `PolynomialFeatures(degree=2)` з `sklearn.preprocessing` для створення поліноміальних взаємодій перед реалізуванням `AdaBoostClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9:** Виконайте об’єднання ознак через `FeatureUnion` для комбінації результатів декількох перетворень (наприклад, PCA та Kernel PCA). Застосуйте ансамблеву модель до нових комбінацій ознак, оцінюючи, як це впливає на результативність моделі.\n",
    "\n",
    "*Технічна примітка:* Використайте `FeatureUnion([('pca', PCA(n_components=5)), ('kpca', KernelPCA(kernel='rbf', n_components=5))])` з `sklearn.pipeline` для створення нових об’єднаних ознак перед реалізуванням `RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10:** Виконайте `Mutual Information Feature Selection` для оцінювання ознак, що найкраще пояснюють цільову змінну, а потім побудуйте модель ансамблю на основі відібраних ознак. Проаналізуйте, як залишення тільки ознак із високою взаємною інформацією покращує точність передбачення.\n",
    "\n",
    "*Технічна примітка:* Використайте `mutual_info_classif` з `SelectKBest` для відбору ознак перед реалізуванням `GradientBoostingClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11:** Виконайте `Partial Least Squares (PLS)` для перетворення набору ознак, щоби зменшити кількість ознак та зберегти основні взаємозв’язки із цільовою змінною. Використайте новий набір зменшених ознак для побудови моделі `AdaBoost` і порівняйте її точність із моделлю на початкових ознаках.\n",
    "\n",
    "*Технічна примітка:* Використайте `PLSRegression(n_components=10)` з `sklearn.cross_decomposition` для перетворення ознак перед реалізуванням `AdaBoostClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12:** Здійсніть `Quantile Transformation` ознак, аби зменшити вплив викривлень розподілу та зробити дані більш нормально розподіленими. Використайте отримані ознаки для побудови моделі `Gradient Boosting` і проаналізуйте, чи впливає таке перетворення на стабільність передбачення.\n",
    "\n",
    "*Технічна примітка:* Використайте `QuantileTransformer(output_distribution='normal')` з `sklearn.preprocessing`, а потім реалізуйте `GradientBoostingClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13:** Використайте `SelectPercentile`, щоб відібрати відсоток найважливіших ознак за їхньою значущістю для цільової змінної. Зберігши, наприклад, 20 % найкращих ознак, побудуйте модель `Random Forest` і порівняйте точність із моделлю на повному наборі ознак.\n",
    "\n",
    "*Технічна примітка:* Використайте `SelectPercentile(score_func=f_classif, percentile=20)` з `sklearn.feature_selection` для відбору ознак, а потім реалізуйте `RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14:** Використайте метод `Kernel PCA` для створення нелінійних компонент, що зменшить розмірність і може підвищити точність моделі. Побудуйте ансамблеву модель, наприклад, `AdaBoost`, на новому наборі ознак і порівняйте її результативність із моделлю на початкових ознаках.\n",
    "\n",
    "*Технічна примітка:* Використайте `KernelPCA(kernel='rbf', n_components=5)` з `sklearn.decomposition` для створення нових компонент перед реалізуванням `AdaBoostClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15:** Застосуйте метод `Stability Selection` до відбору ознак на основі їхньої стабільності на різних випадкових вибірках. Побудуйте модель `Gradient Boosting` на відібраних ознаках і порівняйте її з моделлю, побудованою на повному наборі, щоб оцінити стабільність передбачень.\n",
    "\n",
    "*Технічна примітка:* Використайте `StabilitySelection` з `sklearn.linear_model` разом із `GradientBoostingClassifier` для порівняння моделей на основі стабільності ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16:** Використайте `MaxAbsScaler`, щоб нормалізувати ознаки до діапазону [-1, 1] та зберегти їхні пропорційні значення. Порівняйте результативність моделі `Random Forest` на нормалізованих ознаках із результативністю на початкових ознаках.\n",
    "\n",
    "*Технічна примітка:* Використайте `MaxAbsScaler()` з `sklearn.preprocessing` для нормалізації ознак перед реалізуванням `RandomForestClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17:** Використайте алгоритм `Feature Elimination by Shuffling` для відбору ознак через випадкову перестановку кожної ознаки. Визначте ознаки, які мають найбільший вплив на точність моделі `Gradient Boosting`, та виключіть менш значущі ознаки.\n",
    "\n",
    "*Технічна примітка:* Використайте `permutation_importance` з `sklearn.inspection` для визначення значущих ознак, а потім реалізуйте `GradientBoostingClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18:** Проведіть перетворення `Box-Cox Transformation` на ознаках, що мають позитивні значення, для зменшення їхньої асиметрії. Побудуйте модель `AdaBoost` на перетворених ознаках і проаналізуйте, як така трансформація впливає на результативність.\n",
    "\n",
    "*Технічна примітка:* Використайте `scipy.stats.boxcox` для трансформації ознак, що мають позитивні значення, перед реалізуванням `AdaBoostClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19:** Виконайте `Feature Embedding` на категоріальних ознаках за допомогою нейронної мережі, щоби створити нові вбудовані представлення. Побудуйте ансамблеву модель `Random Forest` на новому наборі вбудованих ознак та оцініть, чи підвищує це точність порівняно з one-hot кодуванням.\n",
    "\n",
    "*Технічна примітка:* Використайте бібліотеку `Keras` для створення вбудованих представлень категоріальних ознак, а потім застосуйте `RandomForestClassifier` до моделювання.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20:** Проведіть `Target Encoding` на категоріальних ознаках для отримання числових представлень на основі середнього значення цільової змінної для кожної категорії. Це дасть можливість створити кращі числові ознаки для побудови моделі `Gradient Boosting`.\n",
    "\n",
    "*Технічна примітка:* Використайте `TargetEncoder` з `category_encoders` для оброблення категоріальних ознак перед реалізуванням `GradientBoostingClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-12T15:55:11.302856400Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "name": "lesson8_part1_kmeans.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
